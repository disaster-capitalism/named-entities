{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0a6dd3e-93b1-4611-9968-c0fe26bca8b0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ff096c7c-98a9-4a35-a370-0937d79f9027",
    "outputId": "d49eef11-3562-443a-c8cc-e56c398c32e2"
   },
   "source": [
    "## Named Entity Recognition (NER) \n",
    "\n",
    "This notebook identifies the main organisations and actors in the OECD corpus of texts. It uses the FLAIR framework for NLP with the 'ner-english-ontonotes-large' model which can be found [here](https://huggingface.co/flair/ner-english-ontonotes-large). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7024aa60-5a59-4360-aac0-cd3f4a078e00",
   "metadata": {},
   "source": [
    "### 1. Import relevant libraries and load the NER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5e621fa-4d0e-43e0-9566-5a7d4336bf20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/huggingface_hub/file_download.py:560: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-25 18:22:09,557 loading file /Users/kodymoodley/.flair/models/ner-english-ontonotes-large/2da6c2cdd76e59113033adf670340bfd820f0301ae2e39204d67ba2dc276cc28.ec1bdb304b6c66111532c3b1fc6e522460ae73f1901848a4d0362cdf9760edb1\n",
      "2022-10-25 18:22:38,576 SequenceTagger predicts: Dictionary with 76 tags: <unk>, O, B-CARDINAL, E-CARDINAL, S-PERSON, S-CARDINAL, S-PRODUCT, B-PRODUCT, I-PRODUCT, E-PRODUCT, B-WORK_OF_ART, I-WORK_OF_ART, E-WORK_OF_ART, B-PERSON, E-PERSON, S-GPE, B-DATE, I-DATE, E-DATE, S-ORDINAL, S-LANGUAGE, I-PERSON, S-EVENT, S-DATE, B-QUANTITY, E-QUANTITY, S-TIME, B-TIME, I-TIME, E-TIME, B-GPE, E-GPE, S-ORG, I-GPE, S-NORP, B-FAC, I-FAC, E-FAC, B-NORP, E-NORP, S-PERCENT, B-ORG, E-ORG, B-LANGUAGE, E-LANGUAGE, I-CARDINAL, I-ORG, S-WORK_OF_ART, I-QUANTITY, B-MONEY\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../util/') # import python preprocessing script\n",
    "\n",
    "from flair.models import SequenceTagger\n",
    "from flair.data import Sentence\n",
    "from flair.tokenization import SegtokSentenceSplitter\n",
    "import numpy as np\n",
    "import csv\n",
    "import os.path\n",
    "\n",
    "# relevant entity types:\n",
    "# ----------------------\n",
    "# FAC\tbuilding name\n",
    "# GPE\tgeo-political entity\n",
    "# LOC\tlocation name\n",
    "# NORP\taffiliation\n",
    "# ORG\torganization name\n",
    "# PERSON\tperson name\n",
    "\n",
    "relevant_ent_types = ['FAC', 'GPE', 'LOC', 'NORP', 'PERSON', 'ORG', 'MISC']\n",
    "\n",
    "flair_18class = SequenceTagger.load('flair/ner-english-ontonotes-large')\n",
    "# flair_12class = SequenceTagger.load('ner-ontonotes-fast')\n",
    "# flair_4class = SequenceTagger.load('ner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad17d3a7-9f93-4776-8e98-4fe8fb465086",
   "metadata": {},
   "source": [
    "### 2. Load the corpus of OECD texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0cdc4deb-295f-4102-9e07-be484b90a62f",
   "metadata": {
    "id": "0cdc4deb-295f-4102-9e07-be484b90a62f"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# get data (full texts of documents including acknowledgements, foreword, executive summary and body)\n",
    "path = Path(os.getcwd())\n",
    "data_dir = os.path.join(path.parents[0], \"data-files\")\n",
    "with open(os.path.join(data_dir, \"data.json\")) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# get data (the structured data which Malte processed into lines with metadata)\n",
    "with open(os.path.join(data_dir, \"studies_on_water_scraped.json\")) as sf:\n",
    "    raw_data = json.load(sf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9983511c-4634-499e-a09a-7ff9d64f1cb4",
   "metadata": {},
   "source": [
    "### 3. Do the NER-specific preprocessing required "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c339fe7-a177-4425-85d1-3f1fc434d39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/kodymoodley/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m preprocess_ner, lookup_correct_docid\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# preprocess data\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdata\u001b[49m:\n\u001b[1;32m      8\u001b[0m     data[key] \u001b[38;5;241m=\u001b[39m preprocess_ner(data[key], stopwords\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "from preprocessing import preprocess_ner, lookup_correct_docid\n",
    "\n",
    "# preprocess data\n",
    "for key in data:\n",
    "    data[key] = preprocess_ner(data[key], stopwords=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b890fa-e3a4-4c6c-956d-3f8828c0c2a9",
   "metadata": {
    "id": "-QmKcjk4MZ2h"
   },
   "source": [
    "### 4. Functions to extract the named entities in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7647a06-b78c-45a5-91a0-5afceb3b327f",
   "metadata": {
    "id": "b7647a06-b78c-45a5-91a0-5afceb3b327f"
   },
   "outputs": [],
   "source": [
    "# function to split sentence list into chunks for batch processing by GPU\n",
    "def split(list_a, chunk_size):\n",
    "    for i in range(0, len(list_a), chunk_size):\n",
    "        yield list_a[i:i + chunk_size]\n",
    "\n",
    "# do NER tagging on a given document\n",
    "def flair_ner(document, model, docid):\n",
    "    results = []\n",
    "    splitter = SegtokSentenceSplitter()\n",
    "    sentences = splitter.split(document)\n",
    "    batches = split(sentences, 20)\n",
    "\n",
    "    for batch in batches:\n",
    "        model.predict(batch)\n",
    "        for sentence in batch:        \n",
    "            for entity in sentence.get_spans('ner'):\n",
    "                if (entity.get_label(\"ner\").value in relevant_ent_types):\n",
    "                    if (len(entity.text) > 1): # one character entities disregarded\n",
    "                        results.append((entity.text.replace('\"', ''), entity.get_label(\"ner\").value, sentence.to_plain_string(), str(entity.start_position) + \":\" + str(entity.end_position), docid))\n",
    "                              \n",
    "    return results\n",
    "\n",
    "# write tagging results to CSV file\n",
    "def write_results_to_file(results, file):\n",
    "    if os.path.exists(file):\n",
    "        # append\n",
    "        with open(file, 'a+', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for item in results:\n",
    "                writer.writerow([str(item[0]), str(item[1]), str(item[2]), str(item[3]), str(item[4]), 'flair - FLERT and XML embeddings'])\n",
    "    else:\n",
    "        # create file from scratch\n",
    "        with open(file, 'w') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['entity', 'entity_type', 'sentence', 'span', 'docid', 'model'])\n",
    "            for item in results:\n",
    "                writer.writerow([str(item[0]), str(item[1]), str(item[2]), str(item[3]), str(item[4]), 'flair - FLERT and XML embeddings'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d2d66a-3dbb-49ab-850a-a880d94b9a10",
   "metadata": {},
   "source": [
    "### 5. Run the NER extraction process on the corpus\n",
    "\n",
    "**Note:** it is highly recommended to run this analysis on a GPU for best performance. For reference, on Google Colab (free edition), using one GPU, it takes approximately 2.5 hours to process the 55 input documents in this corpus. If you are not using a GPU this can take much longer to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7dfd99a-1cb8-4adb-80b3-847fd5385b4e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f7dfd99a-1cb8-4adb-80b3-847fd5385b4e",
    "outputId": "e324561d-372b-45c0-a5d3-9fffedce8ade"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Started entire run at:- 2022-10-25 18:22:55.767029\n",
      "---\n",
      "\n",
      "Started processing Doc (1 / 55) -> DOC ID: 31 at:- 2022-10-25 18:22:55.767244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run the NER tagging on each document in the corpus\n",
    "import datetime;\n",
    "ct = datetime.datetime.now()\n",
    "print()\n",
    "print(\"Started entire run at:-\", ct)\n",
    "print(\"---\")\n",
    "print()\n",
    "\n",
    "idx = 1\n",
    "for key in data:\n",
    "    ct = datetime.datetime.now()\n",
    "    print(\"Started processing Doc (\" + str(idx) + \" / \" + \"55) -> DOC ID: \" + str(lookup_correct_docid(key)) + \" at:-\", ct)\n",
    "    ner_results = flair_ner(data[key], flair_18class, lookup_correct_docid(key))\n",
    "    write_results_to_file(ner_results, os.path.join(data_dir, \"master-ner-results.csv\"))\n",
    "    ct = datetime.datetime.now()\n",
    "    print(\"Finished doc at:-\", ct)\n",
    "    idx += 1\n",
    "\n",
    "print()\n",
    "ct = datetime.datetime.now()\n",
    "print(\"---\")\n",
    "print(\"Finished entire run at:-\", ct)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ner-flair.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
