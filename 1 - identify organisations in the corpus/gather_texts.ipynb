{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8ce7b29-5cc4-424d-9afd-4f20477ae109",
   "metadata": {},
   "source": [
    "## Notebook for extracting OECD corpus texts\n",
    "\n",
    "This notebook generates a single `data.json` file which gathers all the relevant texts from the scraped corpus of OECD documents (`studies_on_water_scraped.json`). This `data.json` file is used in downstream analyses using Named Entity Recognition (NER) and Semantic Role Labelling (SRL). The input data for this script is **not** the raw PDF files of the OECD corpus. The input is rather the scraped, structured data in JSON format which is generated by [this notebook](https://github.com/disaster-capitalism/scrape-corpus/blob/main/scrape_studies_on_water_pdf.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4708fda4-6750-4d45-a207-080969eda8cc",
   "metadata": {},
   "source": [
    "### 1. Import JSON structured data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "778281bd-dee0-4da4-abfd-fc695862a751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "path = Path(os.getcwd())\n",
    "data_dir = os.path.join(path.parents[0], \"data-files\")\n",
    "with open(os.path.join(data_dir, \"studies_on_water_scraped.json\")) as f:\n",
    "    documents = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b586ea9c-e70e-40f7-84a9-76374d474283",
   "metadata": {},
   "source": [
    "### 2. Generate documents\n",
    "\n",
    "Extracts the sections from each document and restructures them into four categories: 1) the forewords, 2) the acknowledgements, 3) the (executive) summaries, and 4) the main body text. Creates a separate document containing the text for each category. The directory `unprocessed/` contains the split documents separated into these categories (ensure you have such a folder created in the `data-files/` directory). The text files are named according the the ID of the document as contained in the JSON input file.\n",
    "\n",
    "Output directory structure under '/path/to/unprocessed/':\n",
    "\n",
    "foreword/ <br/>\n",
    "--- 1.txt <br/>\n",
    "--- 2.txt <br/>\n",
    "--- 3.txt <br/>\n",
    "acknowledgements/ <br/>\n",
    "--- 1.txt <br/>\n",
    "--- 2.txt <br/>\n",
    "--- 3.txt <br/>\n",
    "summary/ <br/>\n",
    "--- 1.txt <br/>\n",
    "--- 2.txt <br/>\n",
    "--- 3.txt <br/>\n",
    "body/ <br/>\n",
    "--- 1.txt <br/>\n",
    "--- 2.txt <br/>\n",
    "--- 3.txt <br/>\n",
    "annexes/ <br/>\n",
    "--- 1.txt <br/>\n",
    "--- 2.txt <br/>\n",
    "--- 3.txt <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "32ad7358-c6a8-4f2f-b3c0-2620dc5e1d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignored_sections = [\"abbreviations\",\"acronyms\",\"glossary\",\"table of contents\",\n",
    "                    \"acronyms and abbreviations\",\"abbreviations and acronyms\",\n",
    "                    \"bibliography\",\"further reading\",\"acronyms and local terms\",\n",
    "                    \"acknowledgements and disclaimers\",\"tables\",\"figures\",\"boxes\",\n",
    "                    \"references and websites\"]\n",
    "\n",
    "# Assumptions or choices made:\n",
    "# ----------------------------\n",
    "# \"advisors and special experts\" and \"Blank Page\" sections\n",
    "# are actually an extension of the \"Acknowledgements\" section\n",
    "# in those documents.\n",
    "#\n",
    "# We merge the Preface and Foreword sections together into one section because they contain similar content\n",
    "    \n",
    "def generate_docs(section):\n",
    "    \"\"\"\n",
    "    Given the OECD corpus, for each file in the corpus, this method generates\n",
    "    a document that contains only the text from the specified section in the file.\n",
    "\n",
    "    :param str section: the section of the document to extract. Options to choose from: \n",
    "    - foreword (preface and foreword sections),\n",
    "    - acknowledgements,\n",
    "    - body (all the chapters and main text in the document),\n",
    "    - summary (executive summary)\n",
    "    - annexes (all the text from the appendices)\n",
    "\n",
    "    \"\"\"\n",
    "    index = 0\n",
    "    for doc in documents:\n",
    "        section_text = \"\"\n",
    "        if (index < 55):\n",
    "            for key in doc:\n",
    "                if (key.lower() not in ignored_sections):\n",
    "                    #key_category = get_key_category(key.lower())\n",
    "                    if (section.lower() == \"foreword\"):\n",
    "                        if (key.lower() == \"foreword\" or key.lower() == \"preface\"):\n",
    "                            for line in doc[key]:\n",
    "                                if ((len(line) - 1 < 0) or (line[len(line)-1] not in [\" \",\"-\"])):\n",
    "                                    tmpline = line + \" \"\n",
    "                                    section_text = section_text + tmpline\n",
    "                                else:\n",
    "                                    section_text = section_text + line\n",
    "                    elif (section.lower() == \"acknowledgements\"):\n",
    "                        if (key.lower().startswith(\"acknowledg\") \n",
    "                                                or (key.lower() == \"advisors and special experts\") \n",
    "                                                or (key.lower() == \"blank page\")):\n",
    "                            for line in doc[key]:\n",
    "                                if ((len(line) - 1 < 0) or (line[len(line)-1] not in [\" \",\"-\"])):\n",
    "                                    tmpline = line + \" \"\n",
    "                                    section_text = section_text + tmpline\n",
    "                                else:\n",
    "                                    section_text = section_text + line           \n",
    "                    elif (section.lower() == \"annexes\"):\n",
    "                        if (key.lower().startswith(\"annex\")):\n",
    "                            for line in doc[key]:\n",
    "                                if ((len(line) - 1 < 0) or (line[len(line)-1] not in [\" \",\"-\"])):\n",
    "                                    tmpline = line + \" \"\n",
    "                                    section_text = section_text + tmpline\n",
    "                                else:\n",
    "                                    section_text = section_text + line \n",
    "                    elif (section.lower() == \"summary\"):\n",
    "                        if (key.lower().startswith(\"executive summary\")):\n",
    "                            for line in doc[key]:\n",
    "                                if ((len(line) - 1 < 0) or (line[len(line)-1] not in [\" \",\"-\"])):\n",
    "                                    tmpline = line + \" \"\n",
    "                                    section_text = section_text + tmpline\n",
    "                                else:\n",
    "                                    section_text = section_text + line\n",
    "                    else:\n",
    "                        for line in doc[key]:\n",
    "                            if ((len(line) - 1 < 0) or (line[len(line)-1] not in [\" \",\"-\"])):\n",
    "                                tmpline = line + \" \"\n",
    "                                section_text = section_text + tmpline\n",
    "                            else:\n",
    "                                section_text = section_text + line\n",
    "                                \n",
    "                # Check whether 'unprocessed/' directory exists\n",
    "                upPath = os.path.join(data_dir, \"unprocessed\")\n",
    "                sPath = os.path.join(upPath, section)\n",
    "                upExist = os.path.exists(upPath)\n",
    "                spExist = os.path.exists(sPath)\n",
    "                if not upExist:\n",
    "                    os.makedirs(upPath)\n",
    "                if not spExist:\n",
    "                    os.makedirs(sPath)\n",
    "                with open(os.path.join(sPath, str(index)+\".txt\"), 'w', encoding='utf-8') as f:\n",
    "                    f.write(section_text)             \n",
    "        index = index + 1\n",
    "    \n",
    "# generate_docs(\"foreword\")\n",
    "# generate_docs(\"summary\")\n",
    "# generate_docs(\"acknowledgements\")\n",
    "generate_docs(\"body\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f234f1-806a-41e2-bf88-d9f4e2867c0f",
   "metadata": {},
   "source": [
    "### 3. Gather the plain text data for each document into a JSON file\n",
    "\n",
    "The JSON file which is generated at the end of this notebook will be the raw input for the NER notebook and other downstream analyses in the study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0d2bd0d3-1820-42fb-86a7-402bb9635ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect data\n",
    "ack_dir = os.path.join(os.path.join(data_dir, \"unprocessed\"), \"acknowledgements\")\n",
    "body_dir = os.path.join(os.path.join(data_dir, \"unprocessed\"), \"body\")\n",
    "summary_dir = os.path.join(os.path.join(data_dir, \"unprocessed\"), \"summary\")\n",
    "foreword_dir = os.path.join(os.path.join(data_dir, \"unprocessed\"), \"foreword\")\n",
    "\n",
    "import json\n",
    "\n",
    "def gather_data():\n",
    "    global data_dir\n",
    "    result = {}\n",
    "#     for filename in os.listdir(foreword_dir):\n",
    "#         f = os.path.join(foreword_dir, filename)\n",
    "\n",
    "#         if (os.path.isfile(f) and (\".DS_Store\" not in f)):\n",
    "#             with open(f, 'r', encoding=\"utf8\") as fp:\n",
    "#                 currentfiledata = fp.read()\n",
    "#             result[f.replace(foreword_dir, \"\").replace(\".txt\", \"\")] = currentfiledata.lower()\n",
    "            \n",
    "#     for filename in os.listdir(ack_dir):\n",
    "#         f = os.path.join(ack_dir, filename)\n",
    "\n",
    "#         if (os.path.isfile(f) and (\".DS_Store\" not in f)):\n",
    "#             with open(f, 'r', encoding=\"utf8\") as fp:\n",
    "#                 currentfiledata = fp.read()\n",
    "#             result[f.replace(ack_dir, \"\").replace(\".txt\", \"\")] = result[f.replace(\"texts/acknowledgements/\", \"\").replace(\".txt\", \"\")] + \" \" + currentfiledata.lower()\n",
    "            \n",
    "#     for filename in os.listdir(summary_dir):\n",
    "#         f = os.path.join(summary_dir, filename)\n",
    "\n",
    "#         if (os.path.isfile(f) and (\".DS_Store\" not in f)):\n",
    "#             with open(f, 'r', encoding=\"utf8\") as fp:\n",
    "#                 currentfiledata = fp.read()\n",
    "#             result[f.replace(summary_dir, \"\").replace(\".txt\", \"\")] = result[f.replace(\"texts/summary/\", \"\").replace(\".txt\", \"\")] + \" \" + currentfiledata.lower()\n",
    "\n",
    "    for filename in os.listdir(body_dir):\n",
    "        f = os.path.join(body_dir, filename)\n",
    "\n",
    "        if (os.path.isfile(f) and (\".DS_Store\" not in f)):\n",
    "            with open(f, 'r', encoding=\"utf8\") as fp:\n",
    "                currentfiledata = fp.read()\n",
    "\n",
    "            result[f.replace(body_dir, \"\").replace(\".txt\", \"\").replace(\"/\",\"\")] = currentfiledata\n",
    "                \n",
    "    with open(os.path.join(data_dir, \"data.json\"), \"w\") as outfile:\n",
    "        json.dump(result, outfile)\n",
    "\n",
    "# invoke the method above, generates a 'data.json' file containing the plain text documents\n",
    "# the keys of the json file are the IDs of the documents as indicated in the input JSON file\n",
    "gather_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
