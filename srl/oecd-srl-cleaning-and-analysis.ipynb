{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70e7af98-1dcb-4f70-91d0-fc16622b9db3",
   "metadata": {},
   "source": [
    "# Semantic Role Labelling: data cleaning and analysis\n",
    "\n",
    "This notebook cleans the raw results from `oecd_semantic_role_labeling.ipynb` and prepares the data for analysis using graph and network analysis tools such as NetworkX, Gephi and iGraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fc9c8d9-7999-4361-abe4-7e79b44167f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe538fe-8da0-4b8a-8028-929f3eaa8bfd",
   "metadata": {},
   "source": [
    "### 1. Load the RAW SRL predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "458290f7-c185-4f93-99dc-1f94793395d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "with open('../data-files/srl_predictions_big.pkl', 'rb') as f:\n",
    "    srl_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0d571c3-f3e2-4d85-b866-0323894987e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95520\n"
     ]
    }
   ],
   "source": [
    "def get_srl_tag_words(sentence):\n",
    "    tokens = re.findall(r'\\[(.*?)\\]', sentence)\n",
    "    verb = None\n",
    "    arg0 = None \n",
    "    arg1 = None\n",
    "    for token in tokens:\n",
    "        if  token.startswith('V:'):\n",
    "            verb = token.replace('V:','').strip()\n",
    "        if  token.startswith('ARG0:'):\n",
    "            arg0 = token.replace('ARG0:','').strip()\n",
    "        if  token.startswith('ARG1:'):\n",
    "            arg1 = token.replace('ARG1:','').strip()\n",
    "\n",
    "    return verb, arg0, arg1\n",
    "\n",
    "triples = set()\n",
    "for i in range(0, len(srl_results)):\n",
    "    for j in range(0, len(srl_results[i][\"verbs\"])):\n",
    "        verb, arg0, arg1 = get_srl_tag_words(srl_results[i][\"verbs\"][j]['description'])\n",
    "        if (verb is not None) and (arg0 is not None) and (arg1 is not None):\n",
    "            triples.add((verb.strip(), arg0.strip(), arg1.strip()))\n",
    "\n",
    "tripleslst = list(triples)\n",
    "\n",
    "print(len(set(tripleslst)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45f7a22-9ff3-4ba9-9b4f-2404f12670de",
   "metadata": {},
   "source": [
    "### 2. Load the named entities dataset from the NER analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78adf438-b0a0-4b18-aab4-e63f7b92fee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>entity_type</th>\n",
       "      <th>sentence</th>\n",
       "      <th>span</th>\n",
       "      <th>docid</th>\n",
       "      <th>model</th>\n",
       "      <th>entity_as_single_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oecd</td>\n",
       "      <td>ORG</td>\n",
       "      <td>preface as part of the oecd programme on water...</td>\n",
       "      <td>23:27</td>\n",
       "      <td>39</td>\n",
       "      <td>flair - FLERT and XML embeddings</td>\n",
       "      <td>oecd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>oecd</td>\n",
       "      <td>ORG</td>\n",
       "      <td>the oecd stands ready to support peru design, ...</td>\n",
       "      <td>4:8</td>\n",
       "      <td>39</td>\n",
       "      <td>flair - FLERT and XML embeddings</td>\n",
       "      <td>oecd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>oecd</td>\n",
       "      <td>ORG</td>\n",
       "      <td>ángel gurría oecd secretary-general gabriel qu...</td>\n",
       "      <td>13:17</td>\n",
       "      <td>39</td>\n",
       "      <td>flair - FLERT and XML embeddings</td>\n",
       "      <td>oecd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>oecd</td>\n",
       "      <td>ORG</td>\n",
       "      <td>ángel gurría oecd secretary-general gabriel qu...</td>\n",
       "      <td>224:228</td>\n",
       "      <td>39</td>\n",
       "      <td>flair - FLERT and XML embeddings</td>\n",
       "      <td>oecd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>oecd</td>\n",
       "      <td>ORG</td>\n",
       "      <td>this report expands the global outreach of oec...</td>\n",
       "      <td>43:47</td>\n",
       "      <td>39</td>\n",
       "      <td>flair - FLERT and XML embeddings</td>\n",
       "      <td>oecd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   entity entity_type                                           sentence  \\\n",
       "0    oecd         ORG  preface as part of the oecd programme on water...   \n",
       "7    oecd         ORG  the oecd stands ready to support peru design, ...   \n",
       "10   oecd         ORG  ángel gurría oecd secretary-general gabriel qu...   \n",
       "14   oecd         ORG  ángel gurría oecd secretary-general gabriel qu...   \n",
       "15   oecd         ORG  this report expands the global outreach of oec...   \n",
       "\n",
       "       span  docid                             model entity_as_single_token  \n",
       "0     23:27     39  flair - FLERT and XML embeddings                   oecd  \n",
       "7       4:8     39  flair - FLERT and XML embeddings                   oecd  \n",
       "10    13:17     39  flair - FLERT and XML embeddings                   oecd  \n",
       "14  224:228     39  flair - FLERT and XML embeddings                   oecd  \n",
       "15    43:47     39  flair - FLERT and XML embeddings                   oecd  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data-files/full-flair-ner-list-oecd-with-single-tokens.csv')\n",
    "orgs_df = df[df['entity_type'] == 'ORG'] # only organisaations\n",
    "orgs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d805f9-874c-4f52-b27f-9e58d7157da4",
   "metadata": {},
   "source": [
    "### 3. Import stopwords and human-specified custom false positive entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "885df311-4157-45dd-a464-ed05ef7265cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "file1 = open('../data-files/replace_stopwords_orgs.txt', 'r')\n",
    "other_stopwords = file1.readlines()\n",
    "other_stopwords2 = []\n",
    "for item in other_stopwords:\n",
    "    other_stopwords2.append(item.replace('\\n',''))\n",
    "\n",
    "irrelevant_tokens = ['technology', 'project', 'region', 'agricultural', \"'s\", 'infrastructure', 'entity', 'state', 'on', 'world', 'working', 'management', 'water_management', 'council', 'task', 'team', 'water', 'climate_change', 'policy', 'covid-19', 'city', 'the', 'et', 'al.', 'x', 'pdf', 'yes', 'abbrev','also','fe',\n",
    "                    'page', 'pp', 'p', 'er', 'doi', 'can', 'b', 'c', 'd', 'e',\n",
    "                    'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'o', 'q', 'r', 's', 'herein', 'furthermore',\n",
    "                    't', 'u', 'v', 'w', 'y', 'z', 'www', 'com', 'org', 'de', 'dx', 'th', 'ii', 'le']\n",
    "\n",
    "stop_words = set(stopwords.words('english')).union(set(irrelevant_tokens))\n",
    "stop_words = stop_words.union(set(other_stopwords2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f1ff61-6f0c-4a79-b95b-089c66ead547",
   "metadata": {},
   "source": [
    "### 4. Clean SRL results: Pass 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7371109-6770-4ff8-a4d4-94556d8eb362",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mhs import hitting_sets\n",
    "\n",
    "\n",
    "def clean_srl_results(tripleslst):\n",
    "    global stop_words\n",
    "    global orgs_df\n",
    "    \n",
    "    highest_quality_results = set()\n",
    "    overall_results = set()\n",
    "    \n",
    "    idx = 1\n",
    "    total = len(list(set(tripleslst)))\n",
    "    for item in list(set(tripleslst)):\n",
    "        if (idx % 2 == 0):\n",
    "            print(idx,\"/\",total, end=' ')\n",
    "        idx+=1\n",
    "        verb = item[0]\n",
    "        arg0 = item[1]\n",
    "        arg1 = item[2]\n",
    "        arg0_tokens = arg0.split()\n",
    "        arg1_tokens = arg1.split()\n",
    "        if (len(arg0_tokens) == 1 and len(arg1_tokens) == 1):\n",
    "            if (arg0_tokens[0].strip() not in stop_words) and (arg1_tokens[0].strip() not in stop_words):\n",
    "                if (arg0_tokens[0].strip() in orgs_df['entity_as_single_token'].tolist()) and (arg1_tokens[0].strip() in orgs_df['entity_as_single_token'].tolist()):\n",
    "                    highest_quality_results.add((verb, arg0_tokens[0].strip(), arg1_tokens[0].strip()))\n",
    "                    overall_results.add((verb, arg0_tokens[0].strip(), arg1_tokens[0].strip()))\n",
    "        else:\n",
    "            if (len(arg0_tokens) > 1) and (len(arg1_tokens) > 1):\n",
    "                relevant_tokens_only_0 = [w for w in arg0_tokens if not w in stop_words]\n",
    "                relevant_tokens_only_0 = [w for w in relevant_tokens_only_0 if w in orgs_df['entity_as_single_token'].tolist()]\n",
    "                valid_ent_0 = False\n",
    "                for item in relevant_tokens_only_0:\n",
    "                    if item in orgs_df['entity_as_single_token'].tolist():\n",
    "                        valid_ent_0 = True\n",
    "                        break\n",
    "                        \n",
    "                new_arg0 = ''\n",
    "                if valid_ent_0:\n",
    "                    new_arg0 = ','.join(list(set(relevant_tokens_only_0)))\n",
    "                \n",
    "                relevant_tokens_only_1 = [w for w in arg1_tokens if not w in stop_words]\n",
    "                relevant_tokens_only_1 = [w for w in relevant_tokens_only_1 if w in orgs_df['entity_as_single_token'].tolist()]\n",
    "                valid_ent_1 = False\n",
    "                for item in relevant_tokens_only_1:\n",
    "                    if item in orgs_df['entity_as_single_token'].tolist():\n",
    "                        valid_ent_1 = True\n",
    "                        break\n",
    "                        \n",
    "                new_arg1 = ''\n",
    "                if valid_ent_1:\n",
    "                    new_arg1 = ','.join(list(set(relevant_tokens_only_1)))\n",
    "                    \n",
    "                if (len(new_arg0) > 0 and len(new_arg1) > 0):\n",
    "                    if (len(relevant_tokens_only_0) > 1) or (len(relevant_tokens_only_1) > 1):\n",
    "                        #hitting sets\n",
    "                        if (len(relevant_tokens_only_0) > 6 or len(relevant_tokens_only_1) > 6):\n",
    "                            print('*', len(relevant_tokens_only_0), len(relevant_tokens_only_1))\n",
    "                        hs = hitting_sets(set(relevant_tokens_only_0[:25]), set(relevant_tokens_only_1[:25]))\n",
    "                        # print()\n",
    "                        # print()\n",
    "                        # print(\"original sets:\", '\\n', set(relevant_tokens_only_0), ',\\n', set(relevant_tokens_only_1))\n",
    "                        # print()\n",
    "                        # print(\"hitting sets:\")\n",
    "                        new_hs = []\n",
    "                        for hs_item in hs:\n",
    "                            if len(hs_item) == 2:\n",
    "                                new_hs.append(list(hs_item))\n",
    "                                print((verb, list(hs_item)[0], list(hs_item)[1]))\n",
    "                                overall_results.add((verb, list(hs_item)[0], list(hs_item)[1]))\n",
    "                        # print(list(new_hs))\n",
    "                        # print()\n",
    "                    else:   # print()\n",
    "                        print((verb, new_arg0, new_arg1))\n",
    "                        overall_results.add((verb, new_arg0, new_arg1))\n",
    "                    \n",
    "            elif (len(arg0_tokens) > 1) and (len(arg1_tokens) == 1):\n",
    "                relevant_tokens_only_0 = [w for w in arg0_tokens if not w in stop_words]\n",
    "                relevant_tokens_only_0 = [w for w in relevant_tokens_only_0 if w in orgs_df['entity_as_single_token'].tolist()]\n",
    "                valid_ent_0 = False\n",
    "                for item in relevant_tokens_only_0:\n",
    "                    if item in orgs_df['entity_as_single_token'].tolist():\n",
    "                        valid_ent_0 = True\n",
    "                        break\n",
    "                        \n",
    "                new_arg0 = ''\n",
    "                if valid_ent_0:\n",
    "                    new_arg0 = ','.join(list(set(relevant_tokens_only_0)))\n",
    "                    \n",
    "                if (len(new_arg0) > 0) and (arg1_tokens[0].strip() not in stop_words) and (arg1_tokens[0].strip() in orgs_df['entity_as_single_token'].tolist()):\n",
    "                    if len(relevant_tokens_only_0) > 1:\n",
    "                        #hitting sets\n",
    "                        if (len(relevant_tokens_only_0) > 6):\n",
    "                            print('*', len(relevant_tokens_only_0))\n",
    "                        \n",
    "                        hs = hitting_sets(set(relevant_tokens_only_0[:50]), set({arg1_tokens[0].strip()}))\n",
    "                        # print()\n",
    "                        # print()\n",
    "                        # print(\"original sets:\", '\\n', set(relevant_tokens_only_0), ',\\n', set({arg1_tokens[0].strip()}))\n",
    "                        # print()\n",
    "                        # print(\"hitting sets:\")\n",
    "                        new_hs = []\n",
    "                        for hs_item in hs:\n",
    "                            if len(hs_item) == 2:\n",
    "                                new_hs.append(list(hs_item))\n",
    "                                print((verb, list(hs_item)[0], list(hs_item)[1]))\n",
    "                                overall_results.add((verb, list(hs_item)[0], list(hs_item)[1]))\n",
    "                        # print(list(new_hs))\n",
    "                        # print()\n",
    "                        # print()\n",
    "                    else:\n",
    "                        print((verb, new_arg0, arg1_tokens[0].strip()))\n",
    "                        overall_results.add((verb, new_arg0, arg1_tokens[0].strip()))\n",
    "                    \n",
    "            elif (len(arg1_tokens) > 1) and (len(arg0_tokens) == 1):\n",
    "                relevant_tokens_only_1 = [w for w in arg1_tokens if not w in stop_words]\n",
    "                relevant_tokens_only_1 = [w for w in relevant_tokens_only_1 if w in orgs_df['entity_as_single_token'].tolist()]\n",
    "                valid_ent_1 = False\n",
    "                for item in relevant_tokens_only_1:\n",
    "                    if item in orgs_df['entity_as_single_token'].tolist():\n",
    "                        valid_ent_1 = True\n",
    "                        break\n",
    "                        \n",
    "                new_arg1 = ''\n",
    "                if valid_ent_1:\n",
    "                    \n",
    "                    new_arg1 = ','.join(list(set(relevant_tokens_only_1)))\n",
    "                    \n",
    "                if (len(new_arg1) > 0) and (arg0_tokens[0].strip() not in stop_words) and (arg0_tokens[0].strip() in orgs_df['entity_as_single_token'].tolist()):\n",
    "                    if len(relevant_tokens_only_1) > 1:\n",
    "                        #hitting sets\n",
    "                        if (len(relevant_tokens_only_1) > 6):\n",
    "                            print('*', len(relevant_tokens_only_1))\n",
    "                        hs = hitting_sets(set({arg0_tokens[0].strip()}), set(relevant_tokens_only_1[:50]))\n",
    "                        # print()\n",
    "                        # print()\n",
    "                        # print(\"original sets:\", '\\n', set({arg0_tokens[0].strip()}), ',\\n', set(relevant_tokens_only_1))\n",
    "                        # print()\n",
    "                        # print(\"hitting sets:\")\n",
    "                        new_hs = []\n",
    "                        for hs_item in hs:\n",
    "                            if len(hs_item) == 2:\n",
    "                                new_hs.append(list(hs_item))\n",
    "                                print((verb, list(hs_item)[0], list(hs_item)[1]))\n",
    "                                overall_results.add((verb, list(hs_item)[0], list(hs_item)[1]))\n",
    "\n",
    "                        # print(list(new_hs))\n",
    "                        # print()\n",
    "                        # print()\n",
    "                    else:\n",
    "                        print((verb, arg0_tokens[0].strip(), new_arg1))\n",
    "                        overall_results.add((verb, arg0_tokens[0].strip(), new_arg1))\n",
    "        print()\n",
    "    return list(highest_quality_results), list(overall_results)\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf896e8-79c4-490d-9ddc-8f25e5f844b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_hq_res, overall_res = clean_srl_results(tripleslst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e70a575-ac22-4a97-9e35-db82c60dfa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(set(new_hq_res)))\n",
    "print(len(set(overall_res)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a79c893-7aba-4cbd-baf9-dd302227bdf5",
   "metadata": {},
   "source": [
    "### 5. Save Pass 1 cleaning results to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e6b422-e6ba-4b4b-a562-b21cbe6f8204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../data-files/srl_predictions_cleaned_singletoken_entities_only.pkl', 'wb') as f:\n",
    "    pickle.dump(new_hq_res, f)\n",
    "    \n",
    "import pickle\n",
    "with open('data-files/srl_predictions_cleaned_pass1.pkl', 'wb') as f:\n",
    "    pickle.dump(overall_res, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda3d37d-828f-4314-912a-98287e3a624a",
   "metadata": {},
   "source": [
    "### 6. Clean SRL results: Pass 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df9fce7-ccd4-4c80-830e-0beee9975df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open('replace_stopwords_orgs.txt', 'r')\n",
    "other_stopwords = file1.readlines()\n",
    "other_stopwords2 = []\n",
    "for item in other_stopwords:\n",
    "    other_stopwords2.append(item.replace('\\n',''))\n",
    "    \n",
    "other_stopwords2 = list(set(other_stopwords2))\n",
    "\n",
    "cleaned_pass2_hq = []\n",
    "cleaned_pass2_overall = []\n",
    "\n",
    "replacements = {\n",
    "    \"solution_water.org\" : \"water.org\",\n",
    "    \"us\" : \"united_states\",\n",
    "    \"eu\" : \"european_union\",\n",
    "    \"un\" : \"united_nations\",\n",
    "    \"ec\" : \"european_commission\",\n",
    "    \"ea\" : \"environment_agency\",\n",
    "    \"environmental_agency\" : \"environment_agency\",\n",
    "    \"eap_task_force\" : \"oecd_eap_task_force\",\n",
    "    \"wb\" : \"world_bank\"\n",
    "}\n",
    "\n",
    "def replace_all(text, dic):\n",
    "    for i, j in dic.items():\n",
    "        if text.strip() == i:\n",
    "            text = text.replace(text, j)\n",
    "    return text\n",
    "\n",
    "def clean_srl_list_pass2(lst, stopwords):\n",
    "    cleaned_lst = []\n",
    "    for item in lst:\n",
    "        verb = item[0]\n",
    "        arg0 = item[1]\n",
    "        arg1 = item[2]\n",
    "\n",
    "        if (arg0 not in stopwords) and (arg1 not in stopwords):\n",
    "            if arg0.startswith('the_'):\n",
    "                arg0 = arg0.replace(arg0[:4], '')\n",
    "\n",
    "            if arg1.startswith('the_'):\n",
    "                arg1 = arg1.replace(arg1[:4], '')\n",
    "\n",
    "            if '_' in arg0:\n",
    "                arg0tokens = arg0.split('_')\n",
    "                # print('arg0:', arg0tokens)\n",
    "                arg0relevant_tokens = []\n",
    "                for token in arg0tokens:\n",
    "                    new_token = replace_all(token, replacements)\n",
    "                    arg0relevant_tokens.append(new_token)\n",
    "                # print('arg0rel:', arg0relevant_tokens)\n",
    "                arg0 = '_'.join(arg0relevant_tokens)\n",
    "            else:\n",
    "                arg0 = replace_all(arg0, replacements)\n",
    "                \n",
    "            if '_' in arg1:\n",
    "                arg1tokens = arg1.split('_')\n",
    "                # print('arg1:', arg1tokens)\n",
    "                arg1relevant_tokens = []\n",
    "                for token2 in arg1tokens:\n",
    "                    new_token2 = replace_all(token2, replacements)\n",
    "                    arg1relevant_tokens.append(new_token2)\n",
    "                # print('arg1rel:', arg1relevant_tokens)\n",
    "                arg1 = '_'.join(arg1relevant_tokens)\n",
    "            else:\n",
    "                arg1 = replace_all(arg1, replacements)\n",
    "        \n",
    "        if (arg0 not in stopwords) and (arg1 not in stopwords):\n",
    "            cleaned_lst.append((verb, arg0, arg1))\n",
    "        \n",
    "    return list(set(cleaned_lst))\n",
    "\n",
    "cleaned_pass2_hq = clean_srl_list_pass2(new_hq_res, other_stopwords2)\n",
    "cleaned_pass2_overall = clean_srl_list_pass2(overall_res, other_stopwords2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c65df2-22c9-4aae-8b7f-fa713f52ccf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item in cleaned_pass2_hq:\n",
    "#     print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fd082e-6516-4293-af84-a6aed3c9f6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item in cleaned_pass2_overall:\n",
    "#     print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaf27ae-08c9-4297-bc5a-62540302f584",
   "metadata": {},
   "source": [
    "### 7. Convert cleaned SRL data to network analysis format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdce14f-5574-4278-87b8-81622379a2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_to_networkx_format(data):\n",
    "    edge_data = []\n",
    "    edge_data_with_labels = {}\n",
    "    \n",
    "    for item in data:\n",
    "        edge_data.append([item[1], item[2]])\n",
    "        \n",
    "    for edge in edge_data:\n",
    "        edge_data_with_labels[(''+edge[0]+'', ''+edge[1]+'')] = []\n",
    "        \n",
    "    for item in data:\n",
    "        edge_data_with_labels[(''+item[1]+'', ''+item[2]+'')].append(item[0])\n",
    "    \n",
    "    for key in edge_data_with_labels:\n",
    "        edge_data_with_labels[key] = ', '.join(list(set(edge_data_with_labels[key])))\n",
    "        \n",
    "    return edge_data, edge_data_with_labels\n",
    "    \n",
    "single_token_graph_edges, single_token_graph_edges_with_labels = convert_data_to_networkx_format(cleaned_pass2_hq)\n",
    "full_graph_edges, full_graph_edges_with_labels = convert_data_to_networkx_format(cleaned_pass2_overall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49a6021-a029-40ff-b703-412f32328daf",
   "metadata": {},
   "source": [
    "### 8. Save cleaned SRL results to file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fbf821-fa12-4146-a098-71fb6d00e3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "header = ['verb', 'source', 'target']\n",
    "\n",
    "with open('../data-files/single_token_actors_data.csv', 'w', encoding='UTF8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "\n",
    "    for item in cleaned_pass2_hq:\n",
    "        rowdata = [item[0], item[1], item[2]]        \n",
    "        writer.writerow(rowdata)\n",
    "        \n",
    "with open('../data-files/single_token_actors_data_(edgesonly).csv', 'w', encoding='UTF8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header[-2:])\n",
    "\n",
    "    for item in cleaned_pass2_hq:\n",
    "        rowdata = [item[1], item[2]]        \n",
    "        writer.writerow(rowdata)\n",
    "    \n",
    "with open('../data-files/full_actors_data.csv', 'w', encoding='UTF8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "\n",
    "    for item in cleaned_pass2_overall:\n",
    "        rowdata = [item[0], item[1], item[2]]        \n",
    "        writer.writerow(rowdata)\n",
    "        \n",
    "        \n",
    "with open('../data-files/full_actors_data_(edgesonly).csv', 'w', encoding='UTF8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header[-2:])\n",
    "\n",
    "    for item in cleaned_pass2_overall:\n",
    "        rowdata = [item[1], item[2]]        \n",
    "        writer.writerow(rowdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59ecf48-5f5d-42ec-a1e4-88f567eb2fe4",
   "metadata": {},
   "source": [
    "### 9. Try to plot using NetworkX \n",
    "\n",
    "This should work for smaller graphs (less than 1000 nodes). But for larger graphs perhaps try Gephi, iGraph etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27734353-1841-43e8-9b65-93d3e9f56d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93aeacbd-b02a-4ca6-adfd-83b28c98db74",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = single_token_graph_edges\n",
    "G = nx.Graph()\n",
    "G.add_edges_from(edges)\n",
    "pos = nx.spring_layout(G, k=2, scale=1, iterations=50)\n",
    "\n",
    "plt.figure()\n",
    "nx.draw(\n",
    "    G, pos, edge_color='black', width=1, linewidths=1,\n",
    "    node_size=1000, node_color='lightblue', alpha=1,\n",
    "    labels={node: node for node in G.nodes()}\n",
    ")\n",
    "# nx.draw_networkx_edge_labels(\n",
    "#     G, pos,\n",
    "#     # edge_labels={('A', 'B'): 'AB', ('B', 'C'): 'BC', ('B', 'D'): 'BD'},\n",
    "#     # edge_labels=single_token_graph_edges_with_labels,\n",
    "#     font_color='red'\n",
    "# )\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2e56d4-8088-4b15-859c-d1b8600be0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # edges = [['A', 'B'], ['B', 'C'], ['B', 'D']]\n",
    "# edges = full_graph_edges\n",
    "# G = nx.Graph()\n",
    "\n",
    "# # G = nx.cycle_graph(80)\n",
    "# G.add_edges_from(edges)\n",
    "# # pos = nx.circular_layout(G)\n",
    "# pos = nx.spring_layout(G, k=0.5, iterations=50)\n",
    "\n",
    "# plt.figure(figsize=(24,24))\n",
    "# nx.draw(\n",
    "#     G, pos, edge_color='black', width=1, linewidths=1,\n",
    "#     node_size=60, node_color='lightblue', alpha=1,\n",
    "#     labels={node: node for node in G.nodes()}\n",
    "# )\n",
    "# # nx.draw_networkx_edge_labels(\n",
    "# #     G, pos,\n",
    "# #     # edge_labels={('A', 'B'): 'AB', ('B', 'C'): 'BC', ('B', 'D'): 'BD'},\n",
    "# #     # edge_labels=single_token_graph_edges_with_labels,\n",
    "# #     font_color='red'\n",
    "# # )\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
