{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fizNciJfq7Dv"
   },
   "source": [
    "# OECD - Semantic Role Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Best to run this notebook on a GPU for a reasonable execution time\n",
    "\n",
    "This notebook generates (Subject, Verb, Object) tuples for the entire OECD corpus of documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qo9rcWrF4ZdZ"
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import re\n",
    "import json\n",
    "import string \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "\n",
    "corpus_path = datapath('../data-files/processed_ngram_ner_data.json')\n",
    "with open(corpus_path, encoding='utf-8') as f:\n",
    "    datajson = json.load(f)\n",
    "\n",
    "corpus = ''\n",
    "for key in datajson:\n",
    "    corpus += datajson[key] + ' '\n",
    "    \n",
    "f = open('../data-files/ngram_replacements.json')\n",
    "ngram_replacements = json.load(f)\n",
    "        \n",
    "def replace_all(text, dic):\n",
    "    for i, j in dic.items():\n",
    "        text = text.replace(i, j)\n",
    "    return text\n",
    "\n",
    "def get_preprocessed_corpus(corpus):\n",
    "    global ngram_replacements\n",
    "    \n",
    "    # split corpus into sentences\n",
    "    sentences = sent_tokenize(corpus)\n",
    "    cleaned_sentences = []\n",
    "    for sentence in sentences:\n",
    "        # replace ngrams with single tokens\n",
    "        cleaned_sentence = replace_all(sentence, ngram_replacements)        \n",
    "        cleaned_sentence = cleaned_sentence.replace('(', '').replace(')', '')\n",
    "        cleaned_sentences.append(cleaned_sentence)\n",
    "            \n",
    "    return cleaned_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F5GuJEMkiqjp"
   },
   "outputs": [],
   "source": [
    "# processed corpus into cleaned list of sentences\n",
    "processed_sentences = get_preprocessed_corpus(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the `processed_sentences` are stored to file (data-files/srl_corpus.pkl), you can also load it with the code in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lGWjjPnS6HFV"
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('/content/drive/MyDrive/srl-allennlp/data-files/srl_corpus.pkl', 'rb') as f:\n",
    "#     processed_sentences = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initialise or prepare SRL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xZAIlPjljrHA",
    "outputId": "3af78cb2-6066-4839-9774-23ddcc994a4f"
   },
   "outputs": [],
   "source": [
    "# initialise the bilstm model for SRL using the allennlp wrapper\n",
    "from allennlp_models import pretrained\n",
    "predictor = pretrained.load_predictor('structured-prediction-srl') # Bilstm model. use 'structured-prediction-srl-bert' as an alternative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you encounter any runtime / installation errors with allennlp, try installing spacy-transformers below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/explosion/spacy-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load the OECD named entities dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "i-HwpjLMi2m6",
    "outputId": "c2b3e4d8-1e92-404b-f761-f09bf0fe1d0d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data-files/full-flair-ner-list-oecd-with-single-tokens.csv')\n",
    "orgs_df = df[df['entity_type'] == 'ORG'] \n",
    "orgs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t8YFmbT3ll3U"
   },
   "source": [
    "## 4. Generate SRL tuples for each sentence in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P5fhaQGVlSt1",
    "outputId": "1c53f8ce-f1ef-46ba-a701-404be896384c"
   },
   "outputs": [],
   "source": [
    "# function to parse SRL output per sentence\n",
    "def get_srl_tag_words(sentence):\n",
    "    tokens = re.findall(r'\\[(.*?)\\]', sentence)\n",
    "    verb = None\n",
    "    arg0 = None \n",
    "    arg1 = None\n",
    "    for token in tokens:\n",
    "        if  token.startswith('V:'):\n",
    "            verb = token.replace('V:','').strip()\n",
    "        if  token.startswith('ARG0:'):\n",
    "            arg0 = token.replace('ARG0:','').strip()\n",
    "        if  token.startswith('ARG1:'):\n",
    "            arg1 = token.replace('ARG1:','').strip()\n",
    "\n",
    "    return verb, arg0, arg1\n",
    "            \n",
    "preds_list = []\n",
    "index = 1\n",
    "for sent in processed_sentences:\n",
    "    print('sentence', index, '/', str(len(processed_sentences)))# \n",
    "    if (len(sent.split()) < 512):\n",
    "        preds = predictor.predict(sent)\n",
    "        for i in range(0, len(preds[\"verbs\"])):\n",
    "            verb, arg0, arg1 = get_srl_tag_words(preds[\"verbs\"][i]['description'])\n",
    "            if (((verb is not None) and (arg0 is not None) and (arg1 is not None)) and (len(verb) > 0 and len(arg0) > 0 and len(arg1) > 0)):\n",
    "                preds_list.append(preds)\n",
    "    index += 1\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Save RAW SRL results to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../data-files/srl_predictions_big.pkl', 'wb') as f:\n",
    "    pickle.dump(preds_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nxo-dKmoqfBA"
   },
   "outputs": [],
   "source": [
    "# with open('/content/drive/MyDrive/srl-allennlp/data-files/srl_predictions.pkl', 'rb') as f:\n",
    "#     srl_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tMnLmZvBqv3-",
    "outputId": "3f83b164-1364-4581-f8be-84946bd850a6"
   },
   "outputs": [],
   "source": [
    "# print(\"number of pairs: \", len(srl_results))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "oecd-semantic-role-labeling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
