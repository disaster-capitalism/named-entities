{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980b6fdf-6e98-4a9e-bd4d-e425c2febfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import json\n",
    "import string \n",
    "\n",
    "corpus_path = datapath('/Users/kodymoodley/Documents/nlesc-projects/disaster-capitalism/embeddings/processed_ngram_ner_data.json')\n",
    "with open(corpus_path, encoding='utf-8') as f:\n",
    "    datajson = json.load(f)\n",
    "\n",
    "corpus = ''\n",
    "for key in datajson:\n",
    "    corpus += datajson[key] + ' '\n",
    "    \n",
    "f = open('ngram_replacements.json')\n",
    "ngram_replacements = json.load(f)\n",
    "        \n",
    "def replace_all(text, dic):\n",
    "    for i, j in dic.items():\n",
    "        text = text.replace(i, j)\n",
    "    return text\n",
    "\n",
    "def get_preprocessed_corpus(corpus):\n",
    "    global ngram_replacements\n",
    "    \n",
    "    output_sentences = []\n",
    "    \n",
    "    # split corpus into sentences\n",
    "    sentences = sent_tokenize(corpus)\n",
    "    cleaned_sentences = []\n",
    "    for sentence in sentences:\n",
    "        # remove numeric tokens\n",
    "        sentence = re.sub(r'\\d+', '', sentence)\n",
    " \n",
    "        # replace ngrams with single tokens\n",
    "        sentence = replace_all(sentence, ngram_replacements)\n",
    "        \n",
    "        # remove URLs\n",
    "        sentence = re.sub('http://\\S+|https://\\S+', '', sentence)\n",
    "        sentence = re.sub('http[s]?://\\S+', '', sentence)\n",
    "        sentence = re.sub(r'http\\S+', '', sentence)\n",
    "        \n",
    "        # replace slashes and dashes used for concatentation with underscores\n",
    "        sentence = sentence.replace(\"/\", \"_\")\n",
    "        sentence = sentence.replace(\"-\", \"_\")\n",
    "        \n",
    "        cleaned_sentences.append(sentence)\n",
    "\n",
    "    # tokenization\n",
    "    for cleaned_sentence in cleaned_sentences:\n",
    "        curr_tokens = word_tokenize(cleaned_sentence)\n",
    "        \n",
    "        # lower casing \n",
    "        for i in range(len(curr_tokens)):\n",
    "            curr_tokens[i] = curr_tokens[i].lower()\n",
    "\n",
    "        # remove punctuation and whitespace characters\n",
    "        tokens_minus_punctuation = []\n",
    "        for token in curr_tokens:\n",
    "            token_contains_punct = False\n",
    "            for chara in token:\n",
    "                # underscores and are allowed\n",
    "                if ((chara in string.punctuation and (chara not in ['_'])) or (chara in ['©','θ','•','','��'])):\n",
    "                    token_contains_punct = True\n",
    "                    break\n",
    "            if not token_contains_punct:\n",
    "                test_whitespace = token.replace(' ','')\n",
    "                # ''', '“', '”' and '-', '‘', '’' are still appearing\n",
    "                test_whitespace = test_whitespace.replace('”','')\n",
    "                test_whitespace = test_whitespace.replace('“','')\n",
    "                test_whitespace = test_whitespace.replace('‘','')\n",
    "                test_whitespace = test_whitespace.replace('’','')\n",
    "                if (len(test_whitespace) > 0):\n",
    "                    token = token.strip()\n",
    "                    token = token.strip('-')\n",
    "                    token = token.replace('-','')\n",
    "                    tokens_minus_punctuation.append(token)\n",
    "\n",
    "        # remove stop words\n",
    "        irrelevant_tokens = ['et', 'al.', 'x', 'pdf', 'yes', 'abbrev','also','fe',\n",
    "                            'page', 'pp', 'p', 'er', 'doi', 'can', 'b', 'c', 'd', 'e',\n",
    "                            'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'o', 'q', 'r', 's', 'herein', 'furthermore',\n",
    "                            't', 'u', 'v', 'w', 'y', 'z', 'www', 'com', 'org', 'de', 'dx', 'th', 'ii', 'le']\n",
    "\n",
    "        stop_words = set(stopwords.words('english')).union(set(irrelevant_tokens))\n",
    "        cleaned_tokens = [w for w in tokens_minus_punctuation if not w in stop_words]\n",
    "        \n",
    "        # lemmatization\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(w) for w in cleaned_tokens]\n",
    "        output_sentences.append(lemmatized_tokens)\n",
    "            \n",
    "    return output_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56b1efd-2ce3-4327-b0f5-e9110d081a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_corpus = get_preprocessed_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9fe021-6321-4b4a-94c3-4119bec50386",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models\n",
    "model_200_10 = gensim.models.Word2Vec(sentences=processed_corpus, sg=1, vector_size=200, window=10, workers=4, min_count=2, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb87f21e-1156-44a1-b781-e3686f47d4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_200_20 = gensim.models.Word2Vec(sentences=processed_corpus, sg=1, vector_size=200, window=20, workers=4, min_count=2, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb910b08-f67e-4e97-9247-00d6e6d279a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_200_30 = gensim.models.Word2Vec(sentences=processed_corpus, sg=1, vector_size=200, window=30, workers=4, min_count=2, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8bf6fb-9274-424b-b639-9b6de6fa3732",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_100_10 = gensim.models.Word2Vec(sentences=processed_corpus, sg=1, vector_size=100, window=10, workers=4, min_count=2, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2577a9-9fea-4e98-8e49-5b5422528335",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_100_20 = gensim.models.Word2Vec(sentences=processed_corpus, sg=1, vector_size=100, window=20, workers=4, min_count=2, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0514e1b9-576d-4103-8f94-c7607881ca35",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_200_40_30 = gensim.models.Word2Vec(sentences=processed_corpus, sg=1, vector_size=200, window=40, workers=4, min_count=2, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097f2a7c-7b34-4b82-94fb-0747eebec8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_100_30 = gensim.models.Word2Vec(sentences=processed_corpus, sg=1, vector_size=100, window=30, workers=4, min_count=2, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ce429d-e35d-404d-8ad2-5be7b5d2f1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_100_40_30 = gensim.models.Word2Vec(sentences=processed_corpus, sg=1, vector_size=100, window=40, workers=4, min_count=2, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d030692-1dba-4346-b3b6-fad26db09818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "with tempfile.NamedTemporaryFile(delete=False) as tmp:\n",
    "    filepath_200_10 = '/Users/kodymoodley/Documents/nlesc-projects/disaster-capitalism/embeddings/models/gensim-oecd-word2vec-200-10.model'\n",
    "    filepath_200_20 = '/Users/kodymoodley/Documents/nlesc-projects/disaster-capitalism/embeddings/models/gensim-oecd-word2vec-200-20.model'\n",
    "    filepath_200_30 = '/Users/kodymoodley/Documents/nlesc-projects/disaster-capitalism/embeddings/models/gensim-oecd-word2vec-200-30.model'\n",
    "    filepath_100_10 = '/Users/kodymoodley/Documents/nlesc-projects/disaster-capitalism/embeddings/models/gensim-oecd-word2vec-100-10.model'\n",
    "    filepath_100_20 = '/Users/kodymoodley/Documents/nlesc-projects/disaster-capitalism/embeddings/models/gensim-oecd-word2vec-100-20.model'\n",
    "    filepath_100_30 = '/Users/kodymoodley/Documents/nlesc-projects/disaster-capitalism/embeddings/models/gensim-oecd-word2vec-100-30.model'\n",
    "    filepath_200_40_30 = '/Users/kodymoodley/Documents/nlesc-projects/disaster-capitalism/embeddings/models/gensim-oecd-word2vec-200-40-30.model'\n",
    "    filepath_100_40_30 = '/Users/kodymoodley/Documents/nlesc-projects/disaster-capitalism/embeddings/models/gensim-oecd-word2vec-100-40-30.model'\n",
    "    model_200_10.save(filepath_200_10)\n",
    "    model_200_20.save(filepath_200_20)\n",
    "    model_200_30.save(filepath_200_30)\n",
    "    model_100_10.save(filepath_100_10)\n",
    "    model_100_20.save(filepath_100_20)\n",
    "    model_100_30.save(filepath_100_30)\n",
    "    model_200_40_30.save(filepath_200_40_30)\n",
    "    model_100_40_30.save(filepath_100_40_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd950d5-3496-41a3-9ba3-6e2468f96002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vec_water = model.wv['water']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcc8d2e-6ef1-40d9-90c3-6916ebb02617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(vec_water)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0662a040-3580-468d-bc32-39e7056cb2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.wv.most_similar(positive=['adaptation'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c1e68e-c1ee-4015-9e90-100b001ff77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, word in enumerate(model.wv.index_to_key):\n",
    "#     if index == 10:\n",
    "#         break\n",
    "#     print(f\"word #{index}/{len(model.wv.index_to_key)} is {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba06676-73b2-495f-b21c-b48c263cba1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_testmodel = gensim.models.Word2Vec.load(filepath_100_10)\n",
    "b_testmodel = gensim.models.Word2Vec.load(filepath_200_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac947e3-96fb-4284-900b-0231097565f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_testmodel.wv.most_similar(positive=['finance'], topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915924eb-cdf9-49bf-afc9-94e0b9898e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_testmodel.wv.most_similar(positive=['finance'], topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc30485-49b0-4d5d-9b3c-b980ea1832f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_testmodel = gensim.models.Word2Vec.load(filepath_100_20)\n",
    "d_testmodel = gensim.models.Word2Vec.load(filepath_200_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3692fc78-1a00-4bbc-8cea-639c3db13b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_testmodel.wv.most_similar(positive=['finance'], topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3076f2b5-b6de-434f-9573-271f0eb760f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_testmodel.wv.most_similar(positive=['finance'], topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04991334-6e99-4898-a176-d07ede459263",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_testmodel = gensim.models.Word2Vec.load(filepath_100_30)\n",
    "f_testmodel = gensim.models.Word2Vec.load(filepath_200_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2f021f-0aea-4330-a82f-35d9d6d06e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_testmodel.wv.most_similar(positive=['rockefeller_foundation'], topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198369f4-a908-442a-addb-e38da3345053",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_testmodel.wv.most_similar(positive=['rockefeller_foundation'], topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f719748-0b68-4f96-9440-e96d39b37b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_testmodel = gensim.models.Word2Vec.load(filepath_200_40_30)\n",
    "h_testmodel = gensim.models.Word2Vec.load(filepath_100_40_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a343e974-13d2-4ea1-b979-d99499ba140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_testmodel.wv.most_similar(positive=['rockefeller_foundation'], topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedd9952-7dbb-4ca4-92e6-11d284ffdb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_testmodel.wv.most_similar(positive=['inei'], topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225034ed-a8c1-4605-b681-16553fa031e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_more_info_about_entity(entity):\n",
    "    global df\n",
    "    new_df = df.drop(['entity', 'model'], axis=1)\n",
    "    relevant_df = new_df[new_df['entity_as_single_token'] == entity].reset_index(drop=True)\n",
    "    types = list(set(relevant_df['entity_type'].tolist()))\n",
    "    docs = list(set(relevant_df['docid'].tolist()))\n",
    "    potential_contexts = relevant_df['sentence'].tolist()\n",
    "    new_contexts = []\n",
    "    spans = relevant_df['span'].tolist()\n",
    "    for i in range(0, len(potential_contexts)):\n",
    "        span_parts = spans[i].split(':')\n",
    "        l_span = int(span_parts[0])\n",
    "        r_span = int(span_parts[1])\n",
    "        left_str = potential_contexts[i][:l_span-1]\n",
    "        right_str = potential_contexts[i][r_span+1:]\n",
    "        left_str_parts = left_str.split()\n",
    "        right_str_parts = right_str.split()\n",
    "        if ((len(left_str_parts) > 3) or (len(right_str_parts) > 3)):\n",
    "            if (len(left_str_parts) <= 3):\n",
    "                new_str = potential_contexts[i][:r_span] + ' ' + ' '.join(right_str_parts[0:3])\n",
    "                new_contexts.append(new_str)\n",
    "            elif (len(right_str_parts) <= 3):\n",
    "                new_str = ' '.join(left_str_parts[-3:]) + ' ' + potential_contexts[i][l_span:]\n",
    "                new_contexts.append(new_str)\n",
    "            else:\n",
    "                new_str = ' '.join(left_str_parts[-3:]) + ' ' + potential_contexts[i][l_span:r_span] +  ' ' + ' '.join(right_str_parts[0:3])\n",
    "                new_contexts.append(new_str)\n",
    "        else:\n",
    "            new_contexts.append(potential_contexts[i])\n",
    "    return {'name' : entity, 'types' : types, 'docs' : docs, 'contexts' : new_contexts}\n",
    "\n",
    "def pretty_print_entity_info(entity_info):\n",
    "    print()\n",
    "    print(\"name\\t\\t:\\t\", entity_info['name']) \n",
    "    print()\n",
    "    print(\"types\\t\\t:\\t\", entity_info['types'])\n",
    "    print()\n",
    "    print(\"documents\\t:\\t\", entity_info['docs'])\n",
    "    print()\n",
    "    print(\"contexts\\t:\\t\", end=\"\")\n",
    "    print(\" 1. \" + entity_info['contexts'][0])\n",
    "    for i in range(1, len(entity_info['contexts'])):\n",
    "        print(\"\\t\\t\\t \" + str(i+1) + \". \" + entity_info['contexts'][i])\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589aae4e-3fd8-48b8-a815-861ac6f26888",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_more_info_about_entity('imta')\n",
    "print(pretty_print_entity_info(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
