{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70e7af98-1dcb-4f70-91d0-fc16622b9db3",
   "metadata": {},
   "source": [
    "## Semantic Role Labelling: data cleaning and analysis\n",
    "\n",
    "This notebook cleans the raw results from `oecd_semantic_role_labeling.ipynb` and prepares the data for analysis using graph and network analysis tools such as NetworkX, Gephi and iGraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fc9c8d9-7999-4361-abe4-7e79b44167f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe538fe-8da0-4b8a-8028-929f3eaa8bfd",
   "metadata": {},
   "source": [
    "### 1. Load the RAW SRL predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "458290f7-c185-4f93-99dc-1f94793395d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "with open('../data-files/srl_predictions_big.pkl', 'rb') as f:\n",
    "    srl_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0d571c3-f3e2-4d85-b866-0323894987e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95520\n"
     ]
    }
   ],
   "source": [
    "def get_srl_tag_words(sentence):\n",
    "    tokens = re.findall(r'\\[(.*?)\\]', sentence)\n",
    "    verb = None\n",
    "    arg0 = None \n",
    "    arg1 = None\n",
    "    for token in tokens:\n",
    "        if  token.startswith('V:'):\n",
    "            verb = token.replace('V:','').strip()\n",
    "        if  token.startswith('ARG0:'):\n",
    "            arg0 = token.replace('ARG0:','').strip()\n",
    "        if  token.startswith('ARG1:'):\n",
    "            arg1 = token.replace('ARG1:','').strip()\n",
    "\n",
    "    return verb, arg0, arg1\n",
    "\n",
    "triples = set()\n",
    "for i in range(0, len(srl_results)):\n",
    "    for j in range(0, len(srl_results[i][\"verbs\"])):\n",
    "        verb, arg0, arg1 = get_srl_tag_words(srl_results[i][\"verbs\"][j]['description'])\n",
    "        if (verb is not None) and (arg0 is not None) and (arg1 is not None):\n",
    "            triples.add((verb.strip(), arg0.strip(), arg1.strip()))\n",
    "\n",
    "tripleslst = list(triples)\n",
    "\n",
    "print(len(set(tripleslst)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45f7a22-9ff3-4ba9-9b4f-2404f12670de",
   "metadata": {},
   "source": [
    "### 2. Load the named entities dataset from the NER analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78adf438-b0a0-4b18-aab4-e63f7b92fee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>entity_type</th>\n",
       "      <th>sentence</th>\n",
       "      <th>span</th>\n",
       "      <th>docid</th>\n",
       "      <th>model</th>\n",
       "      <th>entity_as_single_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OECD</td>\n",
       "      <td>ORG</td>\n",
       "      <td>The OECD, UCLG –Africa, the World Water Counci...</td>\n",
       "      <td>4:8</td>\n",
       "      <td>31</td>\n",
       "      <td>flair - FLERT and XML embeddings</td>\n",
       "      <td>OECD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UCLG – Africa</td>\n",
       "      <td>ORG</td>\n",
       "      <td>The OECD, UCLG –Africa, the World Water Counci...</td>\n",
       "      <td>10:22</td>\n",
       "      <td>31</td>\n",
       "      <td>flair - FLERT and XML embeddings</td>\n",
       "      <td>UCLG_–_Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the World Water Council</td>\n",
       "      <td>ORG</td>\n",
       "      <td>The OECD, UCLG –Africa, the World Water Counci...</td>\n",
       "      <td>24:47</td>\n",
       "      <td>31</td>\n",
       "      <td>flair - FLERT and XML embeddings</td>\n",
       "      <td>World_Water_Council</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the OECD</td>\n",
       "      <td>ORG</td>\n",
       "      <td>This report is published as part of the OECD P...</td>\n",
       "      <td>36:44</td>\n",
       "      <td>31</td>\n",
       "      <td>flair - FLERT and XML embeddings</td>\n",
       "      <td>OECD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Programme on Water Security for Sustainable De...</td>\n",
       "      <td>ORG</td>\n",
       "      <td>This report is published as part of the OECD P...</td>\n",
       "      <td>45:110</td>\n",
       "      <td>31</td>\n",
       "      <td>flair - FLERT and XML embeddings</td>\n",
       "      <td>Programme_on_Water_Security_for_Sustainable_De...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              entity entity_type  \\\n",
       "0                                               OECD         ORG   \n",
       "1                                      UCLG – Africa         ORG   \n",
       "2                            the World Water Council         ORG   \n",
       "4                                           the OECD         ORG   \n",
       "5  Programme on Water Security for Sustainable De...         ORG   \n",
       "\n",
       "                                            sentence    span  docid  \\\n",
       "0  The OECD, UCLG –Africa, the World Water Counci...     4:8     31   \n",
       "1  The OECD, UCLG –Africa, the World Water Counci...   10:22     31   \n",
       "2  The OECD, UCLG –Africa, the World Water Counci...   24:47     31   \n",
       "4  This report is published as part of the OECD P...   36:44     31   \n",
       "5  This report is published as part of the OECD P...  45:110     31   \n",
       "\n",
       "                              model  \\\n",
       "0  flair - FLERT and XML embeddings   \n",
       "1  flair - FLERT and XML embeddings   \n",
       "2  flair - FLERT and XML embeddings   \n",
       "4  flair - FLERT and XML embeddings   \n",
       "5  flair - FLERT and XML embeddings   \n",
       "\n",
       "                              entity_as_single_token  \n",
       "0                                               OECD  \n",
       "1                                      UCLG_–_Africa  \n",
       "2                                World_Water_Council  \n",
       "4                                               OECD  \n",
       "5  Programme_on_Water_Security_for_Sustainable_De...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data-files/master-ner-results-singletokens.csv')\n",
    "orgs_df = df[df['entity_type'] == 'ORG'] # only organisaations\n",
    "orgs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d805f9-874c-4f52-b27f-9e58d7157da4",
   "metadata": {},
   "source": [
    "### 3. Import stopwords and human-specified custom false positive entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "885df311-4157-45dd-a464-ed05ef7265cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "file1 = open('../data-files/replace_stopwords_orgs.txt', 'r')\n",
    "other_stopwords = file1.readlines()\n",
    "other_stopwords2 = []\n",
    "for item in other_stopwords:\n",
    "    other_stopwords2.append(item.replace('\\n',''))\n",
    "\n",
    "irrelevant_tokens = ['technology', 'project', 'region', 'agricultural', \"'s\", 'infrastructure', 'entity', 'state', 'on', 'world', 'working', 'management', 'water_management', 'council', 'task', 'team', 'water', 'climate_change', 'policy', 'covid-19', 'city', 'the', 'et', 'al.', 'x', 'pdf', 'yes', 'abbrev','also','fe',\n",
    "                    'page', 'pp', 'p', 'er', 'doi', 'can', 'b', 'c', 'd', 'e',\n",
    "                    'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'o', 'q', 'r', 's', 'herein', 'furthermore',\n",
    "                    't', 'u', 'v', 'w', 'y', 'z', 'www', 'com', 'org', 'de', 'dx', 'th', 'ii', 'le']\n",
    "\n",
    "stop_words = set(stopwords.words('english')).union(set(irrelevant_tokens))\n",
    "stop_words = stop_words.union(set(other_stopwords2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f1ff61-6f0c-4a79-b95b-089c66ead547",
   "metadata": {},
   "source": [
    "### 4. Clean SRL results: Pass 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7371109-6770-4ff8-a4d4-94556d8eb362",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mhs import hitting_sets\n",
    "\n",
    "\n",
    "def clean_srl_results(tripleslst):\n",
    "    global stop_words\n",
    "    global orgs_df\n",
    "    \n",
    "    highest_quality_results = set()\n",
    "    overall_results = set()\n",
    "    \n",
    "    idx = 1\n",
    "    total = len(list(set(tripleslst)))\n",
    "    for item in list(set(tripleslst)):\n",
    "        if (idx % 2 == 0):\n",
    "            print(idx,\"/\",total, end=' ')\n",
    "        idx+=1\n",
    "        verb = item[0]\n",
    "        arg0 = item[1]\n",
    "        arg1 = item[2]\n",
    "        arg0_tokens = arg0.split()\n",
    "        arg1_tokens = arg1.split()\n",
    "        if (len(arg0_tokens) == 1 and len(arg1_tokens) == 1):\n",
    "            if (arg0_tokens[0].strip() not in stop_words) and (arg1_tokens[0].strip() not in stop_words):\n",
    "                if (arg0_tokens[0].strip() in orgs_df['entity_as_single_token'].tolist()) and (arg1_tokens[0].strip() in orgs_df['entity_as_single_token'].tolist()):\n",
    "                    highest_quality_results.add((verb, arg0_tokens[0].strip(), arg1_tokens[0].strip()))\n",
    "                    overall_results.add((verb, arg0_tokens[0].strip(), arg1_tokens[0].strip()))\n",
    "        else:\n",
    "            if (len(arg0_tokens) > 1) and (len(arg1_tokens) > 1):\n",
    "                relevant_tokens_only_0 = [w for w in arg0_tokens if not w in stop_words]\n",
    "                relevant_tokens_only_0 = [w for w in relevant_tokens_only_0 if w in orgs_df['entity_as_single_token'].tolist()]\n",
    "                valid_ent_0 = False\n",
    "                for item in relevant_tokens_only_0:\n",
    "                    if item in orgs_df['entity_as_single_token'].tolist():\n",
    "                        valid_ent_0 = True\n",
    "                        break\n",
    "                        \n",
    "                new_arg0 = ''\n",
    "                if valid_ent_0:\n",
    "                    new_arg0 = ','.join(list(set(relevant_tokens_only_0)))\n",
    "                \n",
    "                relevant_tokens_only_1 = [w for w in arg1_tokens if not w in stop_words]\n",
    "                relevant_tokens_only_1 = [w for w in relevant_tokens_only_1 if w in orgs_df['entity_as_single_token'].tolist()]\n",
    "                valid_ent_1 = False\n",
    "                for item in relevant_tokens_only_1:\n",
    "                    if item in orgs_df['entity_as_single_token'].tolist():\n",
    "                        valid_ent_1 = True\n",
    "                        break\n",
    "                        \n",
    "                new_arg1 = ''\n",
    "                if valid_ent_1:\n",
    "                    new_arg1 = ','.join(list(set(relevant_tokens_only_1)))\n",
    "                    \n",
    "                if (len(new_arg0) > 0 and len(new_arg1) > 0):\n",
    "                    if (len(relevant_tokens_only_0) > 1) or (len(relevant_tokens_only_1) > 1):\n",
    "                        #hitting sets\n",
    "                        if (len(relevant_tokens_only_0) > 6 or len(relevant_tokens_only_1) > 6):\n",
    "                            print('*', len(relevant_tokens_only_0), len(relevant_tokens_only_1))\n",
    "                        hs = hitting_sets(set(relevant_tokens_only_0[:25]), set(relevant_tokens_only_1[:25]))\n",
    "                        # print()\n",
    "                        # print()\n",
    "                        # print(\"original sets:\", '\\n', set(relevant_tokens_only_0), ',\\n', set(relevant_tokens_only_1))\n",
    "                        # print()\n",
    "                        # print(\"hitting sets:\")\n",
    "                        new_hs = []\n",
    "                        for hs_item in hs:\n",
    "                            if len(hs_item) == 2:\n",
    "                                new_hs.append(list(hs_item))\n",
    "                                print((verb, list(hs_item)[0], list(hs_item)[1]))\n",
    "                                overall_results.add((verb, list(hs_item)[0], list(hs_item)[1]))\n",
    "                        # print(list(new_hs))\n",
    "                        # print()\n",
    "                    else:   # print()\n",
    "                        print((verb, new_arg0, new_arg1))\n",
    "                        overall_results.add((verb, new_arg0, new_arg1))\n",
    "                    \n",
    "            elif (len(arg0_tokens) > 1) and (len(arg1_tokens) == 1):\n",
    "                relevant_tokens_only_0 = [w for w in arg0_tokens if not w in stop_words]\n",
    "                relevant_tokens_only_0 = [w for w in relevant_tokens_only_0 if w in orgs_df['entity_as_single_token'].tolist()]\n",
    "                valid_ent_0 = False\n",
    "                for item in relevant_tokens_only_0:\n",
    "                    if item in orgs_df['entity_as_single_token'].tolist():\n",
    "                        valid_ent_0 = True\n",
    "                        break\n",
    "                        \n",
    "                new_arg0 = ''\n",
    "                if valid_ent_0:\n",
    "                    new_arg0 = ','.join(list(set(relevant_tokens_only_0)))\n",
    "                    \n",
    "                if (len(new_arg0) > 0) and (arg1_tokens[0].strip() not in stop_words) and (arg1_tokens[0].strip() in orgs_df['entity_as_single_token'].tolist()):\n",
    "                    if len(relevant_tokens_only_0) > 1:\n",
    "                        #hitting sets\n",
    "                        if (len(relevant_tokens_only_0) > 6):\n",
    "                            print('*', len(relevant_tokens_only_0))\n",
    "                        \n",
    "                        hs = hitting_sets(set(relevant_tokens_only_0[:50]), set({arg1_tokens[0].strip()}))\n",
    "                        # print()\n",
    "                        # print()\n",
    "                        # print(\"original sets:\", '\\n', set(relevant_tokens_only_0), ',\\n', set({arg1_tokens[0].strip()}))\n",
    "                        # print()\n",
    "                        # print(\"hitting sets:\")\n",
    "                        new_hs = []\n",
    "                        for hs_item in hs:\n",
    "                            if len(hs_item) == 2:\n",
    "                                new_hs.append(list(hs_item))\n",
    "                                print((verb, list(hs_item)[0], list(hs_item)[1]))\n",
    "                                overall_results.add((verb, list(hs_item)[0], list(hs_item)[1]))\n",
    "                        # print(list(new_hs))\n",
    "                        # print()\n",
    "                        # print()\n",
    "                    else:\n",
    "                        print((verb, new_arg0, arg1_tokens[0].strip()))\n",
    "                        overall_results.add((verb, new_arg0, arg1_tokens[0].strip()))\n",
    "                    \n",
    "            elif (len(arg1_tokens) > 1) and (len(arg0_tokens) == 1):\n",
    "                relevant_tokens_only_1 = [w for w in arg1_tokens if not w in stop_words]\n",
    "                relevant_tokens_only_1 = [w for w in relevant_tokens_only_1 if w in orgs_df['entity_as_single_token'].tolist()]\n",
    "                valid_ent_1 = False\n",
    "                for item in relevant_tokens_only_1:\n",
    "                    if item in orgs_df['entity_as_single_token'].tolist():\n",
    "                        valid_ent_1 = True\n",
    "                        break\n",
    "                        \n",
    "                new_arg1 = ''\n",
    "                if valid_ent_1:\n",
    "                    \n",
    "                    new_arg1 = ','.join(list(set(relevant_tokens_only_1)))\n",
    "                    \n",
    "                if (len(new_arg1) > 0) and (arg0_tokens[0].strip() not in stop_words) and (arg0_tokens[0].strip() in orgs_df['entity_as_single_token'].tolist()):\n",
    "                    if len(relevant_tokens_only_1) > 1:\n",
    "                        #hitting sets\n",
    "                        if (len(relevant_tokens_only_1) > 6):\n",
    "                            print('*', len(relevant_tokens_only_1))\n",
    "                        hs = hitting_sets(set({arg0_tokens[0].strip()}), set(relevant_tokens_only_1[:50]))\n",
    "                        # print()\n",
    "                        # print()\n",
    "                        # print(\"original sets:\", '\\n', set({arg0_tokens[0].strip()}), ',\\n', set(relevant_tokens_only_1))\n",
    "                        # print()\n",
    "                        # print(\"hitting sets:\")\n",
    "                        new_hs = []\n",
    "                        for hs_item in hs:\n",
    "                            if len(hs_item) == 2:\n",
    "                                new_hs.append(list(hs_item))\n",
    "                                print((verb, list(hs_item)[0], list(hs_item)[1]))\n",
    "                                overall_results.add((verb, list(hs_item)[0], list(hs_item)[1]))\n",
    "\n",
    "                        # print(list(new_hs))\n",
    "                        # print()\n",
    "                        # print()\n",
    "                    else:\n",
    "                        print((verb, arg0_tokens[0].strip(), new_arg1))\n",
    "                        overall_results.add((verb, arg0_tokens[0].strip(), new_arg1))\n",
    "        print()\n",
    "    return list(highest_quality_results), list(overall_results)\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcf896e8-79c4-490d-9ddc-8f25e5f844b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2 / 95520 \n",
      "\n",
      "4 / 95520 \n",
      "\n",
      "6 / 95520 \n",
      "\n",
      "8 / 95520 \n",
      "\n",
      "10 / 95520 \n",
      "\n",
      "12 / 95520 \n",
      "\n",
      "14 / 95520 \n",
      "\n",
      "16 / 95520 \n",
      "\n",
      "18 / 95520 \n",
      "\n",
      "20 / 95520 \n",
      "\n",
      "22 / 95520 \n",
      "\n",
      "24 / 95520 \n",
      "\n",
      "26 / 95520 \n",
      "\n",
      "28 / 95520 \n",
      "\n",
      "30 / 95520 \n",
      "\n",
      "32 / 95520 \n",
      "\n",
      "34 / 95520 \n",
      "\n",
      "36 / 95520 \n",
      "\n",
      "38 / 95520 \n",
      "\n",
      "40 / 95520 \n",
      "\n",
      "42 / 95520 \n",
      "\n",
      "44 / 95520 \n",
      "\n",
      "46 / 95520 \n",
      "\n",
      "48 / 95520 \n",
      "\n",
      "50 / 95520 \n",
      "\n",
      "52 / 95520 \n",
      "\n",
      "54 / 95520 \n",
      "\n",
      "56 / 95520 \n",
      "\n",
      "58 / 95520 \n",
      "\n",
      "60 / 95520 \n",
      "\n",
      "62 / 95520 \n",
      "\n",
      "64 / 95520 \n",
      "\n",
      "66 / 95520 \n",
      "\n",
      "68 / 95520 \n",
      "\n",
      "70 / 95520 \n",
      "\n",
      "72 / 95520 \n",
      "\n",
      "74 / 95520 \n",
      "\n",
      "76 / 95520 \n",
      "\n",
      "78 / 95520 \n",
      "\n",
      "80 / 95520 \n",
      "\n",
      "82 / 95520 \n",
      "\n",
      "84 / 95520 \n",
      "\n",
      "86 / 95520 \n",
      "\n",
      "88 / 95520 \n",
      "\n",
      "90 / 95520 \n",
      "\n",
      "92 / 95520 \n",
      "\n",
      "94 / 95520 \n",
      "\n",
      "96 / 95520 \n",
      "\n",
      "98 / 95520 \n",
      "\n",
      "100 / 95520 \n",
      "\n",
      "102 / 95520 \n",
      "\n",
      "104 / 95520 \n",
      "\n",
      "106 / 95520 \n",
      "\n",
      "108 / 95520 \n",
      "\n",
      "110 / 95520 \n",
      "\n",
      "112 / 95520 \n",
      "\n",
      "114 / 95520 \n",
      "\n",
      "116 / 95520 \n",
      "\n",
      "118 / 95520 \n",
      "\n",
      "120 / 95520 \n",
      "\n",
      "122 / 95520 \n",
      "\n",
      "124 / 95520 \n",
      "\n",
      "126 / 95520 \n",
      "\n",
      "128 / 95520 \n",
      "\n",
      "130 / 95520 \n",
      "\n",
      "132 / 95520 \n",
      "\n",
      "134 / 95520 \n",
      "\n",
      "136 / 95520 \n",
      "\n",
      "138 / 95520 \n",
      "\n",
      "140 / 95520 \n",
      "\n",
      "142 / 95520 \n",
      "\n",
      "144 / 95520 \n",
      "\n",
      "146 / 95520 \n",
      "\n",
      "148 / 95520 \n",
      "\n",
      "150 / 95520 \n",
      "\n",
      "152 / 95520 \n",
      "\n",
      "154 / 95520 \n",
      "\n",
      "156 / 95520 \n",
      "\n",
      "158 / 95520 \n",
      "\n",
      "160 / 95520 \n",
      "\n",
      "162 / 95520 \n",
      "\n",
      "164 / 95520 \n",
      "\n",
      "166 / 95520 \n",
      "\n",
      "168 / 95520 \n",
      "\n",
      "170 / 95520 \n",
      "\n",
      "172 / 95520 \n",
      "\n",
      "174 / 95520 \n",
      "\n",
      "176 / 95520 \n",
      "\n",
      "178 / 95520 \n",
      "\n",
      "180 / 95520 \n",
      "\n",
      "182 / 95520 \n",
      "\n",
      "184 / 95520 \n",
      "\n",
      "186 / 95520 \n",
      "\n",
      "188 / 95520 \n",
      "\n",
      "190 / 95520 ('•', 'different', 'users')\n",
      "\n",
      "\n",
      "192 / 95520 \n",
      "\n",
      "194 / 95520 \n",
      "\n",
      "196 / 95520 \n",
      "\n",
      "198 / 95520 \n",
      "\n",
      "200 / 95520 \n",
      "\n",
      "202 / 95520 \n",
      "\n",
      "204 / 95520 \n",
      "\n",
      "206 / 95520 \n",
      "\n",
      "208 / 95520 \n",
      "\n",
      "210 / 95520 \n",
      "\n",
      "212 / 95520 \n",
      "\n",
      "214 / 95520 \n",
      "\n",
      "216 / 95520 \n",
      "\n",
      "218 / 95520 \n",
      "\n",
      "220 / 95520 \n",
      "\n",
      "222 / 95520 \n",
      "\n",
      "224 / 95520 \n",
      "\n",
      "226 / 95520 \n",
      "\n",
      "228 / 95520 \n",
      "\n",
      "230 / 95520 \n",
      "\n",
      "232 / 95520 \n",
      "\n",
      "234 / 95520 \n",
      "\n",
      "236 / 95520 \n",
      "\n",
      "238 / 95520 \n",
      "\n",
      "240 / 95520 \n",
      "\n",
      "242 / 95520 \n",
      "\n",
      "244 / 95520 \n",
      "\n",
      "246 / 95520 \n",
      "\n",
      "248 / 95520 \n",
      "\n",
      "250 / 95520 \n",
      "\n",
      "252 / 95520 \n",
      "\n",
      "254 / 95520 \n",
      "\n",
      "256 / 95520 \n",
      "\n",
      "258 / 95520 \n",
      "\n",
      "260 / 95520 \n",
      "\n",
      "262 / 95520 \n",
      "\n",
      "264 / 95520 \n",
      "\n",
      "266 / 95520 \n",
      "\n",
      "268 / 95520 \n",
      "\n",
      "270 / 95520 \n",
      "\n",
      "272 / 95520 \n",
      "\n",
      "274 / 95520 \n",
      "\n",
      "276 / 95520 \n",
      "\n",
      "278 / 95520 \n",
      "\n",
      "280 / 95520 \n",
      "\n",
      "282 / 95520 \n",
      "\n",
      "284 / 95520 \n",
      "\n",
      "286 / 95520 \n",
      "\n",
      "288 / 95520 \n",
      "\n",
      "290 / 95520 \n",
      "\n",
      "292 / 95520 \n",
      "\n",
      "294 / 95520 \n",
      "\n",
      "296 / 95520 \n",
      "\n",
      "298 / 95520 \n",
      "\n",
      "300 / 95520 \n",
      "\n",
      "302 / 95520 \n",
      "\n",
      "304 / 95520 \n",
      "\n",
      "306 / 95520 \n",
      "\n",
      "308 / 95520 \n",
      "\n",
      "310 / 95520 \n",
      "\n",
      "312 / 95520 \n",
      "\n",
      "314 / 95520 \n",
      "\n",
      "316 / 95520 \n",
      "\n",
      "318 / 95520 \n",
      "\n",
      "320 / 95520 \n",
      "\n",
      "322 / 95520 \n",
      "\n",
      "324 / 95520 \n",
      "\n",
      "326 / 95520 \n",
      "\n",
      "328 / 95520 \n",
      "\n",
      "330 / 95520 \n",
      "\n",
      "332 / 95520 \n",
      "\n",
      "334 / 95520 \n",
      "\n",
      "336 / 95520 \n",
      "\n",
      "338 / 95520 \n",
      "\n",
      "340 / 95520 \n",
      "\n",
      "342 / 95520 \n",
      "\n",
      "344 / 95520 \n",
      "\n",
      "346 / 95520 \n",
      "\n",
      "348 / 95520 \n",
      "\n",
      "350 / 95520 \n",
      "\n",
      "352 / 95520 \n",
      "\n",
      "354 / 95520 \n",
      "\n",
      "356 / 95520 \n",
      "\n",
      "358 / 95520 \n",
      "\n",
      "360 / 95520 \n",
      "\n",
      "362 / 95520 \n",
      "\n",
      "364 / 95520 \n",
      "\n",
      "366 / 95520 \n",
      "\n",
      "368 / 95520 \n",
      "\n",
      "370 / 95520 \n",
      "\n",
      "372 / 95520 \n",
      "\n",
      "374 / 95520 \n",
      "\n",
      "376 / 95520 \n",
      "\n",
      "378 / 95520 \n",
      "\n",
      "380 / 95520 \n",
      "\n",
      "382 / 95520 \n",
      "\n",
      "384 / 95520 \n",
      "\n",
      "386 / 95520 \n",
      "\n",
      "388 / 95520 \n",
      "\n",
      "390 / 95520 \n",
      "\n",
      "392 / 95520 \n",
      "\n",
      "394 / 95520 \n",
      "\n",
      "396 / 95520 \n",
      "\n",
      "398 / 95520 \n",
      "\n",
      "400 / 95520 \n",
      "\n",
      "402 / 95520 \n",
      "\n",
      "404 / 95520 \n",
      "\n",
      "406 / 95520 \n",
      "\n",
      "408 / 95520 \n",
      "\n",
      "410 / 95520 \n",
      "\n",
      "412 / 95520 \n",
      "\n",
      "414 / 95520 \n",
      "\n",
      "416 / 95520 \n",
      "\n",
      "418 / 95520 \n",
      "\n",
      "420 / 95520 \n",
      "\n",
      "422 / 95520 \n",
      "\n",
      "424 / 95520 \n",
      "\n",
      "426 / 95520 \n",
      "\n",
      "428 / 95520 \n",
      "\n",
      "430 / 95520 \n",
      "\n",
      "432 / 95520 \n",
      "\n",
      "434 / 95520 \n",
      "\n",
      "436 / 95520 \n",
      "\n",
      "438 / 95520 \n",
      "\n",
      "440 / 95520 \n",
      "\n",
      "442 / 95520 \n",
      "\n",
      "444 / 95520 \n",
      "\n",
      "446 / 95520 \n",
      "\n",
      "448 / 95520 \n",
      "\n",
      "450 / 95520 \n",
      "\n",
      "452 / 95520 \n",
      "\n",
      "454 / 95520 \n",
      "\n",
      "456 / 95520 \n",
      "\n",
      "458 / 95520 \n",
      "\n",
      "460 / 95520 \n",
      "\n",
      "462 / 95520 \n",
      "\n",
      "464 / 95520 \n",
      "\n",
      "466 / 95520 \n",
      "\n",
      "468 / 95520 \n",
      "\n",
      "470 / 95520 \n",
      "\n",
      "472 / 95520 \n",
      "\n",
      "474 / 95520 \n",
      "\n",
      "476 / 95520 \n",
      "\n",
      "478 / 95520 \n",
      "\n",
      "480 / 95520 \n",
      "\n",
      "482 / 95520 \n",
      "\n",
      "484 / 95520 \n",
      "\n",
      "486 / 95520 \n",
      "\n",
      "488 / 95520 \n",
      "\n",
      "490 / 95520 \n",
      "\n",
      "492 / 95520 \n",
      "\n",
      "494 / 95520 \n",
      "\n",
      "496 / 95520 \n",
      "\n",
      "498 / 95520 \n",
      "\n",
      "500 / 95520 \n",
      "\n",
      "502 / 95520 \n",
      "\n",
      "504 / 95520 \n",
      "\n",
      "506 / 95520 \n",
      "\n",
      "508 / 95520 \n",
      "\n",
      "510 / 95520 \n",
      "\n",
      "512 / 95520 \n",
      "\n",
      "514 / 95520 \n",
      "\n",
      "516 / 95520 \n",
      "\n",
      "518 / 95520 \n",
      "\n",
      "520 / 95520 \n",
      "\n",
      "522 / 95520 \n",
      "\n",
      "524 / 95520 \n",
      "\n",
      "526 / 95520 \n",
      "\n",
      "528 / 95520 \n",
      "\n",
      "530 / 95520 \n",
      "\n",
      "532 / 95520 \n",
      "\n",
      "534 / 95520 \n",
      "\n",
      "536 / 95520 \n",
      "\n",
      "538 / 95520 \n",
      "\n",
      "540 / 95520 \n",
      "\n",
      "542 / 95520 \n",
      "\n",
      "544 / 95520 \n",
      "\n",
      "546 / 95520 \n",
      "\n",
      "548 / 95520 \n",
      "\n",
      "550 / 95520 \n",
      "\n",
      "552 / 95520 \n",
      "\n",
      "554 / 95520 \n",
      "\n",
      "556 / 95520 \n",
      "\n",
      "558 / 95520 \n",
      "\n",
      "560 / 95520 \n",
      "\n",
      "562 / 95520 \n",
      "\n",
      "564 / 95520 \n",
      "\n",
      "566 / 95520 \n",
      "\n",
      "568 / 95520 \n",
      "\n",
      "570 / 95520 \n",
      "\n",
      "572 / 95520 \n",
      "\n",
      "574 / 95520 \n",
      "\n",
      "576 / 95520 \n",
      "\n",
      "578 / 95520 \n",
      "\n",
      "580 / 95520 \n",
      "\n",
      "582 / 95520 \n",
      "\n",
      "584 / 95520 \n",
      "\n",
      "586 / 95520 \n",
      "\n",
      "588 / 95520 \n",
      "\n",
      "590 / 95520 \n",
      "\n",
      "592 / 95520 \n",
      "\n",
      "594 / 95520 \n",
      "\n",
      "596 / 95520 \n",
      "\n",
      "598 / 95520 \n",
      "\n",
      "600 / 95520 \n",
      "\n",
      "602 / 95520 \n",
      "\n",
      "604 / 95520 \n",
      "\n",
      "606 / 95520 \n",
      "\n",
      "608 / 95520 \n",
      "\n",
      "610 / 95520 \n",
      "\n",
      "612 / 95520 \n",
      "\n",
      "614 / 95520 \n",
      "\n",
      "616 / 95520 \n",
      "\n",
      "618 / 95520 \n",
      "\n",
      "620 / 95520 \n",
      "\n",
      "622 / 95520 \n",
      "\n",
      "624 / 95520 \n",
      "\n",
      "626 / 95520 \n",
      "\n",
      "628 / 95520 \n",
      "\n",
      "630 / 95520 \n",
      "\n",
      "632 / 95520 \n",
      "\n",
      "634 / 95520 \n",
      "\n",
      "636 / 95520 \n",
      "\n",
      "638 / 95520 \n",
      "\n",
      "640 / 95520 \n",
      "\n",
      "642 / 95520 \n",
      "\n",
      "644 / 95520 \n",
      "\n",
      "646 / 95520 \n",
      "\n",
      "648 / 95520 \n",
      "\n",
      "650 / 95520 \n",
      "\n",
      "652 / 95520 \n",
      "\n",
      "654 / 95520 \n",
      "\n",
      "656 / 95520 \n",
      "\n",
      "658 / 95520 \n",
      "\n",
      "660 / 95520 \n",
      "\n",
      "662 / 95520 \n",
      "\n",
      "664 / 95520 \n",
      "\n",
      "666 / 95520 \n",
      "\n",
      "668 / 95520 \n",
      "\n",
      "670 / 95520 \n",
      "\n",
      "672 / 95520 \n",
      "\n",
      "674 / 95520 \n",
      "\n",
      "676 / 95520 \n",
      "\n",
      "678 / 95520 \n",
      "\n",
      "680 / 95520 \n",
      "\n",
      "682 / 95520 \n",
      "\n",
      "684 / 95520 \n",
      "\n",
      "686 / 95520 \n",
      "\n",
      "688 / 95520 \n",
      "\n",
      "690 / 95520 \n",
      "\n",
      "692 / 95520 \n",
      "\n",
      "694 / 95520 \n",
      "\n",
      "696 / 95520 \n",
      "\n",
      "698 / 95520 \n",
      "\n",
      "700 / 95520 \n",
      "\n",
      "702 / 95520 \n",
      "\n",
      "704 / 95520 \n",
      "\n",
      "706 / 95520 \n",
      "\n",
      "708 / 95520 \n",
      "\n",
      "710 / 95520 \n",
      "\n",
      "712 / 95520 \n",
      "\n",
      "714 / 95520 \n",
      "\n",
      "716 / 95520 \n",
      "\n",
      "718 / 95520 \n",
      "\n",
      "720 / 95520 \n",
      "\n",
      "722 / 95520 \n",
      "\n",
      "724 / 95520 \n",
      "\n",
      "726 / 95520 \n",
      "\n",
      "728 / 95520 \n",
      "\n",
      "730 / 95520 \n",
      "\n",
      "732 / 95520 \n",
      "\n",
      "734 / 95520 \n",
      "\n",
      "736 / 95520 \n",
      "\n",
      "738 / 95520 \n",
      "\n",
      "740 / 95520 \n",
      "\n",
      "742 / 95520 \n",
      "\n",
      "744 / 95520 \n",
      "\n",
      "746 / 95520 \n",
      "\n",
      "748 / 95520 \n",
      "\n",
      "750 / 95520 \n",
      "\n",
      "752 / 95520 \n",
      "\n",
      "754 / 95520 \n",
      "\n",
      "756 / 95520 \n",
      "\n",
      "758 / 95520 \n",
      "\n",
      "760 / 95520 \n",
      "\n",
      "762 / 95520 \n",
      "\n",
      "764 / 95520 \n",
      "\n",
      "766 / 95520 \n",
      "\n",
      "768 / 95520 \n",
      "\n",
      "770 / 95520 \n",
      "\n",
      "772 / 95520 \n",
      "\n",
      "774 / 95520 \n",
      "\n",
      "776 / 95520 \n",
      "\n",
      "778 / 95520 \n",
      "\n",
      "780 / 95520 \n",
      "\n",
      "782 / 95520 \n",
      "\n",
      "784 / 95520 \n",
      "\n",
      "786 / 95520 \n",
      "\n",
      "788 / 95520 \n",
      "\n",
      "790 / 95520 \n",
      "\n",
      "792 / 95520 \n",
      "\n",
      "794 / 95520 \n",
      "\n",
      "796 / 95520 \n",
      "\n",
      "798 / 95520 \n",
      "\n",
      "800 / 95520 \n",
      "\n",
      "802 / 95520 \n",
      "\n",
      "804 / 95520 \n",
      "\n",
      "806 / 95520 \n",
      "\n",
      "808 / 95520 \n",
      "\n",
      "810 / 95520 \n",
      "\n",
      "812 / 95520 \n",
      "\n",
      "814 / 95520 \n",
      "\n",
      "816 / 95520 \n",
      "\n",
      "818 / 95520 \n",
      "\n",
      "820 / 95520 \n",
      "\n",
      "822 / 95520 \n",
      "\n",
      "824 / 95520 \n",
      "\n",
      "826 / 95520 \n",
      "\n",
      "828 / 95520 \n",
      "\n",
      "830 / 95520 \n",
      "\n",
      "832 / 95520 \n",
      "\n",
      "834 / 95520 \n",
      "\n",
      "836 / 95520 \n",
      "\n",
      "838 / 95520 \n",
      "\n",
      "840 / 95520 \n",
      "\n",
      "842 / 95520 \n",
      "\n",
      "844 / 95520 \n",
      "\n",
      "846 / 95520 \n",
      "\n",
      "848 / 95520 \n",
      "\n",
      "850 / 95520 \n",
      "\n",
      "852 / 95520 \n",
      "\n",
      "854 / 95520 \n",
      "\n",
      "856 / 95520 \n",
      "\n",
      "858 / 95520 \n",
      "\n",
      "860 / 95520 \n",
      "\n",
      "862 / 95520 \n",
      "\n",
      "864 / 95520 \n",
      "\n",
      "866 / 95520 \n",
      "\n",
      "868 / 95520 \n",
      "\n",
      "870 / 95520 \n",
      "\n",
      "872 / 95520 \n",
      "\n",
      "874 / 95520 \n",
      "\n",
      "876 / 95520 \n",
      "\n",
      "878 / 95520 \n",
      "\n",
      "880 / 95520 \n",
      "\n",
      "882 / 95520 \n",
      "\n",
      "884 / 95520 \n",
      "\n",
      "886 / 95520 \n",
      "\n",
      "888 / 95520 \n",
      "\n",
      "890 / 95520 \n",
      "\n",
      "892 / 95520 \n",
      "\n",
      "894 / 95520 \n",
      "\n",
      "896 / 95520 \n",
      "\n",
      "898 / 95520 \n",
      "\n",
      "900 / 95520 \n",
      "\n",
      "902 / 95520 \n",
      "\n",
      "904 / 95520 \n",
      "\n",
      "906 / 95520 \n",
      "\n",
      "908 / 95520 \n",
      "\n",
      "910 / 95520 \n",
      "\n",
      "912 / 95520 \n",
      "\n",
      "914 / 95520 \n",
      "\n",
      "916 / 95520 \n",
      "\n",
      "918 / 95520 \n",
      "\n",
      "920 / 95520 \n",
      "\n",
      "922 / 95520 \n",
      "\n",
      "924 / 95520 \n",
      "\n",
      "926 / 95520 \n",
      "\n",
      "928 / 95520 \n",
      "\n",
      "930 / 95520 \n",
      "\n",
      "932 / 95520 \n",
      "\n",
      "934 / 95520 \n",
      "\n",
      "936 / 95520 \n",
      "\n",
      "938 / 95520 \n",
      "\n",
      "940 / 95520 \n",
      "\n",
      "942 / 95520 \n",
      "\n",
      "944 / 95520 \n",
      "\n",
      "946 / 95520 \n",
      "\n",
      "948 / 95520 \n",
      "\n",
      "950 / 95520 \n",
      "\n",
      "952 / 95520 \n",
      "\n",
      "954 / 95520 \n",
      "\n",
      "956 / 95520 \n",
      "\n",
      "958 / 95520 \n",
      "\n",
      "960 / 95520 \n",
      "\n",
      "962 / 95520 \n",
      "\n",
      "964 / 95520 \n",
      "\n",
      "966 / 95520 \n",
      "\n",
      "968 / 95520 \n",
      "\n",
      "970 / 95520 \n",
      "\n",
      "972 / 95520 \n",
      "\n",
      "974 / 95520 \n",
      "\n",
      "976 / 95520 \n",
      "\n",
      "978 / 95520 \n",
      "\n",
      "980 / 95520 \n",
      "\n",
      "982 / 95520 \n",
      "\n",
      "984 / 95520 \n",
      "\n",
      "986 / 95520 \n",
      "\n",
      "988 / 95520 \n",
      "\n",
      "990 / 95520 \n",
      "\n",
      "992 / 95520 \n",
      "\n",
      "994 / 95520 \n",
      "\n",
      "996 / 95520 \n",
      "\n",
      "998 / 95520 \n",
      "\n",
      "1000 / 95520 \n",
      "\n",
      "1002 / 95520 \n",
      "\n",
      "1004 / 95520 \n",
      "\n",
      "1006 / 95520 \n",
      "\n",
      "1008 / 95520 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_hq_res, overall_res = clean_srl_results(tripleslst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e70a575-ac22-4a97-9e35-db82c60dfa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(set(new_hq_res)))\n",
    "print(len(set(overall_res)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a79c893-7aba-4cbd-baf9-dd302227bdf5",
   "metadata": {},
   "source": [
    "### 5. Save Pass 1 cleaning results to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e6b422-e6ba-4b4b-a562-b21cbe6f8204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../data-files/srl_predictions_cleaned_singletoken_entities_only.pkl', 'wb') as f:\n",
    "    pickle.dump(new_hq_res, f)\n",
    "    \n",
    "import pickle\n",
    "with open('../data-files/srl_predictions_cleaned_pass1.pkl', 'wb') as f:\n",
    "    pickle.dump(overall_res, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda3d37d-828f-4314-912a-98287e3a624a",
   "metadata": {},
   "source": [
    "### 6. Clean SRL results: Pass 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df9fce7-ccd4-4c80-830e-0beee9975df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open('../data-files/replace_stopwords_orgs.txt', 'r')\n",
    "other_stopwords = file1.readlines()\n",
    "other_stopwords2 = []\n",
    "for item in other_stopwords:\n",
    "    other_stopwords2.append(item.replace('\\n',''))\n",
    "    \n",
    "other_stopwords2 = list(set(other_stopwords2))\n",
    "\n",
    "cleaned_pass2_hq = []\n",
    "cleaned_pass2_overall = []\n",
    "\n",
    "replacements = {\n",
    "    \"solution_water.org\" : \"water.org\",\n",
    "    \"us\" : \"united_states\",\n",
    "    \"eu\" : \"european_union\",\n",
    "    \"un\" : \"united_nations\",\n",
    "    \"ec\" : \"european_commission\",\n",
    "    \"ea\" : \"environment_agency\",\n",
    "    \"environmental_agency\" : \"environment_agency\",\n",
    "    \"eap_task_force\" : \"oecd_eap_task_force\",\n",
    "    \"wb\" : \"world_bank\"\n",
    "}\n",
    "\n",
    "def replace_all(text, dic):\n",
    "    for i, j in dic.items():\n",
    "        if text.strip() == i:\n",
    "            text = text.replace(text, j)\n",
    "    return text\n",
    "\n",
    "def clean_srl_list_pass2(lst, stopwords):\n",
    "    cleaned_lst = []\n",
    "    for item in lst:\n",
    "        verb = item[0]\n",
    "        arg0 = item[1]\n",
    "        arg1 = item[2]\n",
    "\n",
    "        if (arg0 not in stopwords) and (arg1 not in stopwords):\n",
    "            if arg0.startswith('the_'):\n",
    "                arg0 = arg0.replace(arg0[:4], '')\n",
    "\n",
    "            if arg1.startswith('the_'):\n",
    "                arg1 = arg1.replace(arg1[:4], '')\n",
    "\n",
    "            if '_' in arg0:\n",
    "                arg0tokens = arg0.split('_')\n",
    "                # print('arg0:', arg0tokens)\n",
    "                arg0relevant_tokens = []\n",
    "                for token in arg0tokens:\n",
    "                    new_token = replace_all(token, replacements)\n",
    "                    arg0relevant_tokens.append(new_token)\n",
    "                # print('arg0rel:', arg0relevant_tokens)\n",
    "                arg0 = '_'.join(arg0relevant_tokens)\n",
    "            else:\n",
    "                arg0 = replace_all(arg0, replacements)\n",
    "                \n",
    "            if '_' in arg1:\n",
    "                arg1tokens = arg1.split('_')\n",
    "                # print('arg1:', arg1tokens)\n",
    "                arg1relevant_tokens = []\n",
    "                for token2 in arg1tokens:\n",
    "                    new_token2 = replace_all(token2, replacements)\n",
    "                    arg1relevant_tokens.append(new_token2)\n",
    "                # print('arg1rel:', arg1relevant_tokens)\n",
    "                arg1 = '_'.join(arg1relevant_tokens)\n",
    "            else:\n",
    "                arg1 = replace_all(arg1, replacements)\n",
    "        \n",
    "        if (arg0 not in stopwords) and (arg1 not in stopwords):\n",
    "            cleaned_lst.append((verb, arg0, arg1))\n",
    "        \n",
    "    return list(set(cleaned_lst))\n",
    "\n",
    "cleaned_pass2_hq = clean_srl_list_pass2(new_hq_res, other_stopwords2)\n",
    "cleaned_pass2_overall = clean_srl_list_pass2(overall_res, other_stopwords2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaf27ae-08c9-4297-bc5a-62540302f584",
   "metadata": {},
   "source": [
    "### 7. Convert cleaned SRL data to network analysis format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdce14f-5574-4278-87b8-81622379a2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_to_networkx_format(data):\n",
    "    edge_data = []\n",
    "    edge_data_with_labels = {}\n",
    "    \n",
    "    for item in data:\n",
    "        edge_data.append([item[1], item[2]])\n",
    "        \n",
    "    for edge in edge_data:\n",
    "        edge_data_with_labels[(''+edge[0]+'', ''+edge[1]+'')] = []\n",
    "        \n",
    "    for item in data:\n",
    "        edge_data_with_labels[(''+item[1]+'', ''+item[2]+'')].append(item[0])\n",
    "    \n",
    "    for key in edge_data_with_labels:\n",
    "        edge_data_with_labels[key] = ', '.join(list(set(edge_data_with_labels[key])))\n",
    "        \n",
    "    return edge_data, edge_data_with_labels\n",
    "    \n",
    "single_token_graph_edges, single_token_graph_edges_with_labels = convert_data_to_networkx_format(cleaned_pass2_hq)\n",
    "full_graph_edges, full_graph_edges_with_labels = convert_data_to_networkx_format(cleaned_pass2_overall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49a6021-a029-40ff-b703-412f32328daf",
   "metadata": {},
   "source": [
    "### 8. Save cleaned SRL results to file \n",
    "\n",
    "To import into Gephi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fbf821-fa12-4146-a098-71fb6d00e3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "header = ['verb', 'source', 'target']\n",
    "\n",
    "with open('../data-files/single_token_actors_data.csv', 'w', encoding='UTF8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "\n",
    "    for item in cleaned_pass2_hq:\n",
    "        rowdata = [item[0], item[1], item[2]]        \n",
    "        writer.writerow(rowdata)\n",
    "        \n",
    "with open('../data-files/single_token_actors_data_edgesonly.csv', 'w', encoding='UTF8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header[-2:])\n",
    "\n",
    "    for item in cleaned_pass2_hq:\n",
    "        rowdata = [item[1], item[2]]        \n",
    "        writer.writerow(rowdata)\n",
    "    \n",
    "with open('../data-files/full_actors_data.csv', 'w', encoding='UTF8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "\n",
    "    for item in cleaned_pass2_overall:\n",
    "        rowdata = [item[0], item[1], item[2]]        \n",
    "        writer.writerow(rowdata)\n",
    "        \n",
    "        \n",
    "with open('../data-files/full_actors_data_edgesonly.csv', 'w', encoding='UTF8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header[-2:])\n",
    "\n",
    "    for item in cleaned_pass2_overall:\n",
    "        rowdata = [item[1], item[2]]        \n",
    "        writer.writerow(rowdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59ecf48-5f5d-42ec-a1e4-88f567eb2fe4",
   "metadata": {},
   "source": [
    "### 9. Try to plot using NetworkX \n",
    "\n",
    "This should work for smaller graphs (less than 1000 nodes). But for larger graphs perhaps try Gephi, iGraph etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27734353-1841-43e8-9b65-93d3e9f56d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93aeacbd-b02a-4ca6-adfd-83b28c98db74",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = single_token_graph_edges\n",
    "G = nx.Graph()\n",
    "G.add_edges_from(edges)\n",
    "pos = nx.spring_layout(G, k=2, scale=1, iterations=50)\n",
    "\n",
    "plt.figure()\n",
    "nx.draw(\n",
    "    G, pos, edge_color='black', width=1, linewidths=1,\n",
    "    node_size=1000, node_color='lightblue', alpha=1,\n",
    "    labels={node: node for node in G.nodes()}\n",
    ")\n",
    "# nx.draw_networkx_edge_labels(\n",
    "#     G, pos,\n",
    "#     # edge_labels={('A', 'B'): 'AB', ('B', 'C'): 'BC', ('B', 'D'): 'BD'},\n",
    "#     # edge_labels=single_token_graph_edges_with_labels,\n",
    "#     font_color='red'\n",
    "# )\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
