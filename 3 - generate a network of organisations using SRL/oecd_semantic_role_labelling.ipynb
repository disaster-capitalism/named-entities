{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fizNciJfq7Dv"
   },
   "source": [
    "## OECD - Semantic Role Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** It is highly recommended to run this notebook on a GPU for a reasonable execution time.\n",
    "\n",
    "This notebook generates (Subject, Verb, Object) tuples for the entire OECD corpus of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Qo9rcWrF4ZdZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 8] nodename nor\n",
      "[nltk_data]     servname provided, or not known>\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import re\n",
    "import json\n",
    "import string \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))  \n",
    "irrelevant_tokens = ['et', 'al.', 'x', 'pdf', 'yes', 'abbrev','also','fe',\n",
    "                            'page', 'pp', 'p', 'er', 'doi', 'can', 'b', 'c', 'd', 'e',\n",
    "                            'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'o', 'q', 'r', 's', 'herein', 'furthermore',\n",
    "                            't', 'u', 'v', 'w', 'y', 'z', 'www', 'com', 'org', 'de', 'dx', 'th', 'ii', 'le']\n",
    "\n",
    "stop_words_list = []\n",
    "stop_words_list.extend(list(stop_words))\n",
    "stop_words_list.extend(list(irrelevant_tokens))\n",
    "stop_words_list = list(set(stop_words_list))\n",
    "\n",
    "with open('../data-files/processed_ngram_ner_data.json', encoding='utf-8') as f:\n",
    "    datajson = json.load(f)\n",
    "\n",
    "corpus = ''\n",
    "corpus_doclist = []\n",
    "for key in datajson:\n",
    "    word_tokens = word_tokenize(datajson[key])\n",
    "    filtered_doc = [w for w in word_tokens if not w.lower() in stop_words_list]\n",
    "    corpus_doclist.append(' '.join(filtered_doc))\n",
    "    corpus += datajson[key] + ' '\n",
    "    \n",
    "f = open('../data-files/ngram_replacements.json')\n",
    "ngram_replacements = json.load(f)\n",
    "        \n",
    "def replace_all(text, dic):\n",
    "    for i, j in dic.items():\n",
    "        text = text.replace(i, j)\n",
    "    return text\n",
    "\n",
    "def get_preprocessed_corpus(corpus):\n",
    "    global ngram_replacements\n",
    "    \n",
    "    # split corpus into sentences\n",
    "    sentences = sent_tokenize(corpus)\n",
    "    cleaned_sentences = []\n",
    "    for sentence in sentences:\n",
    "        # replace ngrams with single tokens\n",
    "        cleaned_sentence = replace_all(sentence, ngram_replacements)        \n",
    "        cleaned_sentence = cleaned_sentence.replace('(', '').replace(')', '')\n",
    "        cleaned_sentences.append(cleaned_sentence)\n",
    "            \n",
    "    return cleaned_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "F5GuJEMkiqjp"
   },
   "outputs": [],
   "source": [
    "# processed corpus into cleaned list of sentences\n",
    "processed_sentences = get_preprocessed_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save corpus to file\n",
    "import pickle\n",
    "with open('../data-files/srl_corpus.pkl', 'wb') as f:\n",
    "    pickle.dump(processed_sentences, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the `processed_sentences` are stored to file (data-files/srl_corpus.pkl), you can also load it with the code in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lGWjjPnS6HFV"
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('../data-files/srl_corpus.pkl', 'rb') as f:\n",
    "#     processed_sentences = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initialise or prepare SRL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xZAIlPjljrHA",
    "outputId": "3af78cb2-6066-4839-9774-23ddcc994a4f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "roberta-rte is not a registered model.\n",
      "lerc is not a registered model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b57690dcd5b40a9b276b70db1f11eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading:   0%|          | 0/54185577 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_core_web_sm' (3.0.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.4.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# initialise the bilstm model for SRL using the allennlp wrapper\n",
    "from allennlp_models import pretrained\n",
    "predictor = pretrained.load_predictor('structured-prediction-srl') # Bilstm model. use 'structured-prediction-srl-bert' as an alternative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you encounter any runtime / installation errors with allennlp, try installing spacy-transformers below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/explosion/spacy-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t8YFmbT3ll3U"
   },
   "source": [
    "### 3. Generate SRL tuples for each sentence in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P5fhaQGVlSt1",
    "outputId": "1c53f8ce-f1ef-46ba-a701-404be896384c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence 1 / 127867\n",
      "sentence 2 / 127867\n",
      "sentence 3 / 127867\n",
      "sentence 4 / 127867\n",
      "sentence 5 / 127867\n",
      "sentence 6 / 127867\n",
      "sentence 7 / 127867\n",
      "sentence 8 / 127867\n",
      "sentence 9 / 127867\n",
      "sentence 10 / 127867\n",
      "sentence 11 / 127867\n",
      "sentence 12 / 127867\n",
      "sentence 13 / 127867\n",
      "sentence 14 / 127867\n",
      "sentence 15 / 127867\n",
      "sentence 16 / 127867\n",
      "sentence 17 / 127867\n",
      "sentence 18 / 127867\n",
      "sentence 19 / 127867\n",
      "sentence 20 / 127867\n",
      "sentence 21 / 127867\n",
      "sentence 22 / 127867\n",
      "sentence 23 / 127867\n",
      "sentence 24 / 127867\n",
      "sentence 25 / 127867\n",
      "sentence 26 / 127867\n",
      "sentence 27 / 127867\n",
      "sentence 28 / 127867\n",
      "sentence 29 / 127867\n",
      "sentence 30 / 127867\n",
      "sentence 31 / 127867\n",
      "sentence 32 / 127867\n",
      "sentence 33 / 127867\n",
      "sentence 34 / 127867\n",
      "sentence 35 / 127867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# function to parse SRL output per sentence\n",
    "def get_srl_tag_words(sentence):\n",
    "    tokens = re.findall(r'\\[(.*?)\\]', sentence)\n",
    "    verb = None\n",
    "    arg0 = None \n",
    "    arg1 = None\n",
    "    for token in tokens:\n",
    "        if  token.startswith('V:'):\n",
    "            verb = token.replace('V:','').strip()\n",
    "        if  token.startswith('ARG0:'):\n",
    "            arg0 = token.replace('ARG0:','').strip()\n",
    "        if  token.startswith('ARG1:'):\n",
    "            arg1 = token.replace('ARG1:','').strip()\n",
    "\n",
    "    return verb, arg0, arg1\n",
    "            \n",
    "preds_list = []\n",
    "index = 1\n",
    "for sent in processed_sentences:\n",
    "    print('sentence', index, '/', str(len(processed_sentences)))# \n",
    "    if (len(sent.split()) < 512):\n",
    "        preds = predictor.predict(sent)\n",
    "        for i in range(0, len(preds[\"verbs\"])):\n",
    "            verb, arg0, arg1 = get_srl_tag_words(preds[\"verbs\"][i]['description'])\n",
    "            if (((verb is not None) and (arg0 is not None) and (arg1 is not None)) and (len(verb) > 0 and len(arg0) > 0 and len(arg1) > 0)):\n",
    "                preds_list.append(preds)\n",
    "    index += 1\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Save RAW SRL results to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../data-files/srl_predictions_big.pkl', 'wb') as f:\n",
    "    pickle.dump(preds_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nxo-dKmoqfBA"
   },
   "outputs": [],
   "source": [
    "# with open('../data-files/srl_predictions_big.pkl', 'rb') as f:\n",
    "#     srl_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tMnLmZvBqv3-",
    "outputId": "3f83b164-1364-4581-f8be-84946bd850a6"
   },
   "outputs": [],
   "source": [
    "# print(\"number of pairs: \", len(srl_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Exploratory analysis of document term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# v = TfidfVectorizer()\n",
    "# x = v.fit_transform(corpus_doclist)\n",
    "# dtm = v.transform(corpus_doclist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Doc 1     Doc 2     Doc 3     Doc 4     Doc 5     Doc 6  \\\n",
      "water        0.447218  0.616300  0.185371  0.515143  0.312592  0.381654   \n",
      "oecd         0.151829  0.153343  0.026068  0.159456  0.143150  0.001332   \n",
      "countries    0.103725  0.082532  0.062997  0.017483  0.096407  0.015985   \n",
      "moldova      0.000000  0.000000  0.355790  0.000000  0.000907  0.217215   \n",
      "wss          0.012327  0.000000  0.470274  0.000000  0.000000  0.063357   \n",
      "development  0.044346  0.044928  0.057928  0.053654  0.032970  0.094581   \n",
      "public       0.048104  0.026371  0.045619  0.060889  0.009599  0.037966   \n",
      "management   0.048104  0.063486  0.014482  0.090127  0.028380  0.105904   \n",
      "may          0.018791  0.088392  0.087617  0.065712  0.112683  0.022646   \n",
      "risk         0.026307  0.195829  0.005793  0.084099  0.097659  0.037299   \n",
      "\n",
      "                Doc 7     Doc 8     Doc 9    Doc 10  \n",
      "water        0.277393  0.425145  0.242719  0.358689  \n",
      "oecd         0.093125  0.172985  0.166716  0.138756  \n",
      "countries    0.242719  0.014970  0.014097  0.127429  \n",
      "moldova      0.118463  0.000000  0.000000  0.000000  \n",
      "wss          0.118608  0.000000  0.000000  0.000000  \n",
      "development  0.085199  0.071523  0.061906  0.088728  \n",
      "public       0.060432  0.055222  0.115230  0.169905  \n",
      "management   0.025758  0.062208  0.055776  0.084009  \n",
      "may          0.035665  0.036926  0.020227  0.086841  \n",
      "risk         0.012879  0.008982  0.017775  0.077401  \n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# # Select the first five rows from the data set\n",
    "# td = pd.DataFrame(x.todense()).iloc[:10]  \n",
    "# td.columns = v.get_feature_names_out()\n",
    "# term_document_matrix = td.T\n",
    "# term_document_matrix.columns = ['Doc '+str(i) for i in range(1, 11)]\n",
    "# term_document_matrix['total_count'] = term_document_matrix.sum(axis=1)\n",
    "\n",
    "# # Top 25 words \n",
    "# term_document_matrix = term_document_matrix.sort_values(by ='total_count',ascending=False)[:25] \n",
    "\n",
    "# # Print the first 10 rows \n",
    "# print(term_document_matrix.drop(columns=['total_count']).head(10))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "oecd-semantic-role-labeling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
