{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fizNciJfq7Dv"
   },
   "source": [
    "## OECD - Semantic Role Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** It is highly recommended to run this notebook on a GPU for a reasonable execution time.\n",
    "\n",
    "This notebook generates (Subject, Verb, Object) tuples for the entire OECD corpus of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qo9rcWrF4ZdZ"
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import re\n",
    "import json\n",
    "import string \n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# nltk.download('punkt')\n",
    "from pathlib import Path\n",
    "import os\n",
    "path = Path(os.getcwd())\n",
    "data_dir = os.path.join(path.parents[0], \"data-files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, \"processed_ngram_ner_data.json\"), encoding='utf-8') as f:\n",
    "    datajson = json.load(f)\n",
    "\n",
    "processed_sentences = []\n",
    "for key in datajson:\n",
    "    sentences = sent_tokenize(datajson[key])\n",
    "    for sentence in sentences:\n",
    "        processed_sentences.append(sentence)    \n",
    "        \n",
    "print(len(processed_sentences))\n",
    "\n",
    "processed_sentences = list(set(processed_sentences))\n",
    "\n",
    "print(len(processed_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F5GuJEMkiqjp"
   },
   "outputs": [],
   "source": [
    "# processed corpus into cleaned list of sentences\n",
    "# processed_sentences = get_preprocessed_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save corpus to file\n",
    "import pickle\n",
    "with open(os.path.join(data_dir, \"srl_corpus_new.pkl\"), 'wb') as f:\n",
    "    pickle.dump(processed_sentences, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the `processed_sentences` are stored to file (data-files/srl_corpus.pkl), you can also load it with the code in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lGWjjPnS6HFV"
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(os.path.join(data_dir, \"srl_corpus_new.pkl\"), 'rb') as f:\n",
    "#     processed_sentences = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initialise or prepare SRL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xZAIlPjljrHA",
    "outputId": "3af78cb2-6066-4839-9774-23ddcc994a4f"
   },
   "outputs": [],
   "source": [
    "# initialise the bilstm model for SRL using the allennlp wrapper\n",
    "from allennlp_models import pretrained\n",
    "\n",
    "\n",
    "predictor = pretrained.load_predictor('structured-prediction-srl') # Bilstm model. use 'structured-prediction-srl-bert' as an alternative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you encounter any runtime / installation errors with allennlp, try installing spacy-transformers below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/explosion/spacy-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t8YFmbT3ll3U"
   },
   "source": [
    "### 3. Generate SRL tuples for each sentence in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P5fhaQGVlSt1",
    "outputId": "1c53f8ce-f1ef-46ba-a701-404be896384c"
   },
   "outputs": [],
   "source": [
    "# function to parse SRL output per sentence\n",
    "def get_srl_tag_words(sentence):\n",
    "    tokens = re.findall(r'\\[(.*?)\\]', sentence)\n",
    "    verb = None\n",
    "    arg0 = None \n",
    "    arg1 = None\n",
    "    for token in tokens:\n",
    "        if  token.startswith('V:'):\n",
    "            verb = token.replace('V:','').strip()\n",
    "        if  token.startswith('ARG0:'):\n",
    "            arg0 = token.replace('ARG0:','').strip()\n",
    "        if  token.startswith('ARG1:'):\n",
    "            arg1 = token.replace('ARG1:','').strip()\n",
    "\n",
    "    return verb, arg0, arg1\n",
    "            \n",
    "preds_list = []\n",
    "index = 1\n",
    "for sent in processed_sentences:\n",
    "    # print('sentence', index, '/', str(len(processed_sentences)))#\n",
    "    if (len(sent.split()) < 512):\n",
    "        preds = predictor.predict(sent)\n",
    "        for i in range(0, len(preds[\"verbs\"])):\n",
    "            verb, arg0, arg1 = get_srl_tag_words(preds[\"verbs\"][i]['description'])\n",
    "            if (((verb is not None) and (arg0 is not None) and (arg1 is not None)) and (len(verb) > 0 and len(arg0) > 0 and len(arg1) > 0)):\n",
    "                preds_list.append(preds)\n",
    "    index += 1\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Save RAW SRL results to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(os.path.join(data_dir, \"srl_predictions_big.pkl\"), 'wb') as f:\n",
    "    pickle.dump(preds_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nxo-dKmoqfBA"
   },
   "outputs": [],
   "source": [
    "# with open(os.path.join(data_dir, \"srl_predictions_big.pkl\"), 'rb') as f:\n",
    "#     srl_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tMnLmZvBqv3-",
    "outputId": "3f83b164-1364-4581-f8be-84946bd850a6"
   },
   "outputs": [],
   "source": [
    "# print(\"number of pairs: \", len(srl_results))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "oecd-semantic-role-labeling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
