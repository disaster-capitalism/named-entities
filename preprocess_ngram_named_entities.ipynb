{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23902e8a-b486-41ae-bdd9-1705f0e273fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import os.path\n",
    "import json\n",
    "\n",
    "df = pd.read_csv('full-flair-ner-list-oecd-corpus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a4810b6-2b8e-4f22-8097-181608614938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data (full texts of documents including acknowledgements, foreword, executive summary and body)\n",
    "f = open('data.json')\n",
    "data = json.load(f)\n",
    "\n",
    "# get data (the structured data which Malte processed into lines with metadata)\n",
    "sf = open('studies_on_water_scraped.json')\n",
    "malte_data = json.load(sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b70858d4-91a7-4858-92e2-9e72c47fefb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import preprocess_string, strip_tags, strip_multiple_whitespaces\n",
    "\n",
    "# define preprocessing steps\n",
    "def preprocess(text):\n",
    "    # remove URLs\n",
    "    text = re.sub('http://\\S+|https://\\S+', '', text)\n",
    "    text = re.sub('http[s]?://\\S+', '', text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub('^(http:\\/\\/www\\.|https:\\/\\/www\\.|http:\\/\\/|https:\\/\\/)?[a-z0-9]+([\\-\\.]{1}[a-z0-9]+)*\\.[a-z]{2,5}(:[0-9]{1,5})?(\\/.*)?$', '', text)\n",
    "    \n",
    "    # remove HTML / XML-like tags in text and multiple whitespaces\n",
    "    CUSTOM_FILTERS = [strip_tags, strip_multiple_whitespaces]\n",
    "    text_tokens = preprocess_string(text, CUSTOM_FILTERS)\n",
    "    \n",
    "    # remove niche irrelevant characters\n",
    "    irrelevant_tokens = ['et', 'al.', 'x', 'pdf', 'yes', 'abbrev','fe',\n",
    "                            'page', 'pp', 'p', 'er', 'doi', 'can', 'b', 'c', 'd', 'e',\n",
    "                            'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'o', 'q', 'r', 's',\n",
    "                            't', 'u', 'v', 'w', 'y', 'z','www', 'com', 'org', 'de', 'dx', 'th', 'ii', 'le']\n",
    "\n",
    "    tokens_without_sw = [word.strip() for word in text_tokens if not word.strip() in irrelevant_tokens]\n",
    "    text = ' '.join(tokens_without_sw)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9cdfef9c-1509-4661-a5ac-79703a10c71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data\n",
    "for key in data:\n",
    "    data[key] = preprocess(data[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2303fc71-662b-4dc0-a801-80014cf37ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to lookup correct ID for document in studies_on_water_scraped.json\n",
    "# before this, I was using the INDEX of the document in the JSON array of this file as its ID.\n",
    "def lookup_correct_docid(old_key):\n",
    "    global malte_data\n",
    "    return malte_data[int(old_key)]['meta']['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc350b7d-46b4-49c5-b2be-078e139e577d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_all(text, dic):\n",
    "    for i, j in dic.items():\n",
    "        text = text.replace(i, j)\n",
    "    return text\n",
    "\n",
    "def process(datafr, doctext, docid):\n",
    "    global entity_dict\n",
    "    # print(docid)\n",
    "    # first filter the rows pertaining to the given docid\n",
    "    docid_datafr = datafr[datafr['docid'] == docid]\n",
    "    # then filter only for organisations and persons\n",
    "    docid_org_per_datafr = docid_datafr[docid_datafr['entity_type'].isin(['ORG', 'PERSON', 'NORP', 'LOC', 'FAC', 'GPE'])]\n",
    "    \n",
    "    # print(\"before: \", len(docid_org_per_datafr))\n",
    "    # old_count = len(docid_org_per_datafr)\n",
    "    # v = docid_org_per_datafr[['entity']]\n",
    "    # docid_org_per_datafr = docid_org_per_datafr[v.replace(v.stack().value_counts()).gt(2).all(1)]\n",
    "    # print(\"after: \", len(docid_org_per_datafr))\n",
    "    # new_count = len(docid_org_per_datafr)\n",
    "    # loop through each entity mention (row) in the dataframe\n",
    "    docid_org_per_datafr = docid_org_per_datafr.reset_index()  # make sure indexes pair with number of rows\n",
    "    unique_sentences = pd.unique(docid_org_per_datafr['sentence'])\n",
    "    \n",
    "    for sentence in unique_sentences:\n",
    "        curr_sent_df = docid_org_per_datafr[docid_org_per_datafr['sentence'] == sentence]\n",
    "        curr_sent_entities = curr_sent_df['entity'].tolist()\n",
    "        # if 'rockefeller' in sentence:\n",
    "        #     print()\n",
    "        #     print()\n",
    "        #     print(curr_sent_entities)\n",
    "        #     print()\n",
    "        #     print()\n",
    "        replace_patterns = {}\n",
    "        for entity in curr_sent_entities:\n",
    "            if isinstance(entity, str):\n",
    "                named_entity_tokens = entity.strip().replace('\"', '').replace(\"'\", '').replace(\",\",'').replace('’','').replace('‘','').replace('“','').replace('”','').split()\n",
    "                if (len(named_entity_tokens) > 1):\n",
    "                    # form single token from multiple ones\n",
    "                    single_token_entity = '_'.join(named_entity_tokens)\n",
    "                    replace_patterns[entity] = single_token_entity\n",
    "                    entity_dict[entity] = single_token_entity\n",
    "                \n",
    "        new_sentence = replace_all(sentence, replace_patterns)\n",
    "        doctext = doctext.replace(sentence, new_sentence)\n",
    "\n",
    "    return doctext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a3d3237-eb86-428b-8082-4150317bf553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Started entire run at:- 2022-07-12 08:35:34.616352\n",
      "---\n",
      "\n",
      "\n",
      "---\n",
      "Finished entire run at:- 2022-07-12 08:35:51.082673\n"
     ]
    }
   ],
   "source": [
    "# run the NER tagging on each document in the corpus\n",
    "import datetime;\n",
    "ct = datetime.datetime.now()\n",
    "print()\n",
    "print(\"Started entire run at:-\", ct)\n",
    "print(\"---\")\n",
    "print()\n",
    "\n",
    "entity_dict = {}\n",
    "processed_data = {}\n",
    "for key in data:\n",
    "    ct = datetime.datetime.now()\n",
    "    # print(\"Started processing Doc (\" + str(key) + \"-\" + str(lookup_correct_docid(key)) + \" / \" + \"55) at:-\", ct)\n",
    "    processed_data[lookup_correct_docid(key)] = process(df, data[key], lookup_correct_docid(key))\n",
    "    ct = datetime.datetime.now()\n",
    "    # print(\"Finished processing Doc (\" + str(key) + \"-\" + str(lookup_correct_docid(key)) + \" / \" + \"55) at:-\", ct)\n",
    "\n",
    "with open('processed_ngram_ner_data.json', 'w') as fp:\n",
    "    json.dump(processed_data, fp)\n",
    "    \n",
    "with open('entity_dict.json', 'w') as fp:\n",
    "    json.dump(entity_dict, fp)\n",
    "    \n",
    "print()\n",
    "ct = datetime.datetime.now()\n",
    "print(\"---\")\n",
    "print(\"Finished entire run at:-\", ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5e51ba6-58d1-402b-b778-4218e121227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare a list that is to be converted into a column\n",
    "single_tokens = []\n",
    "\n",
    "for item in df['entity'].tolist():\n",
    "    if isinstance(item, str):\n",
    "        named_entity_tokens = item.strip().replace('\"', '').replace(\"'\", '').replace(\",\",'').replace('’','').replace('‘','').replace('“','').replace('”','').split()\n",
    "        if (len(named_entity_tokens) > 1):\n",
    "            single_tokens.append(entity_dict[item])\n",
    "        elif (len(named_entity_tokens) == 1):\n",
    "            single_tokens.append(item)\n",
    "        else:\n",
    "            single_tokens.append(None)\n",
    "    else:\n",
    "        single_tokens.append(None)\n",
    "                          \n",
    "df['entity_as_single_token'] = single_tokens\n",
    "\n",
    "df.to_csv('full-flair-ner-list-oecd-with-single-tokens.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79035d55-2f0d-4b90-9404-a9ee60d46344",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
